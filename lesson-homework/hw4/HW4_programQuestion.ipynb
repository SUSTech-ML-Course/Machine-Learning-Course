{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Southern University of Science and Technology-Department of Computer Science and Engineering\n",
    "\n",
    "Course: Machine Learning(CS 405)-Professor: Qi Hao\n",
    "\n",
    "## Homework #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import libraries that you might require.\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you need to implement a convolutional neural network and the learning algorithms from scratch for an MNIST classification task.**Only numpy is allowed to use**.\n",
    "\n",
    "The network structure is shown as below:\n",
    "1. `Conv2d (in_channels=1, out_channels=3, kernel_size=(3,3), stride=1, padding=(1,1))`\n",
    "2. `LeakyReLU (negative_slope=0.01)`\n",
    "3. `MaxPool2d (kernel_size=(2,2), stride=2)`\n",
    "4. `Flatten`\n",
    "5. `FullyConnected`\n",
    "6. `Softmax`\n",
    "\n",
    "You also need to:\n",
    "- Compute the number of input nodes of the fully connected layer according to the size of input images.\n",
    "- Initialize the weights randomly by $0.1 \\times N(0, 1)$ where $N$ is the normal distribution. The random seed is given.\n",
    "- Normalize the inputs to $[0, 1]$ and encode the training labels to one-hot representation.\n",
    "- Load the training samples with `batch_size=200`, and the validation samples with `batch_size=100`.\n",
    "- Use the [Adam optimization algorithm](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam) for training. The hyperparameters should be:`learning_rate=0.02`,`beta1=0.9`,`beta2=0.999`, and `eps=1e-8`.\n",
    "- Use [L2 norm](https://en.wikipedia.org/wiki/Norm_(mathematics)) to clip the gradients during optimization to avoid gradient explosion. The hyperparameters should be:`max_norm=10`,`eps=1e-6`.\n",
    "- Train the network for five epochs. At the end of each epoch, compute the classification accuracy of the model on the test set, and print it to standard output.\n",
    "\n",
    "by completing the TODOs in the provided code, there are 7 places to fill out, namely:\n",
    "- `compute_output_dims` in `Conv2d`,\n",
    "- `backward` of `Conv2d`,\n",
    "- `compute_output_dims` in `MaxPool2d`,\n",
    "- `forward` in `MaxPool2d`,\n",
    "- `backward` in `MaxPool2d`,\n",
    "- `forward` in `SoftMax`,\n",
    "- in `main`, insert a `FullyConnected` model into pipeline with given parameters.\n",
    "\n",
    "Backward functions are implemented in a way that they can be called in reverse order of forward functions. You can refer to the provided code for more details.\n",
    "```python\n",
    "def backward(self, da):\n",
    "    \"\"\"\n",
    "    Perform the backward pass for the convolutional layer.\n",
    "\n",
    "    Args:\n",
    "    - da (numpy.ndarray): Gradient from the next layer, shape (batch_size, c_out, h_out, w_out)\n",
    "\n",
    "    Returns:\n",
    "    - out (numpy.ndarray): Gradient with respect to the input x, shape (batch_size, c_in, h_in, w_in)\n",
    "    \"\"\"\n",
    "    # Retrieve the shape of the gradients and parameters\n",
    "    batch_size, c_out, h_out, w_out = da.shape\n",
    "    h_f, w_f = self.kernel_size\n",
    "    h_p, w_p = self.padding\n",
    "    stride = self.stride\n",
    "\n",
    "    # Initialize gradients\n",
    "    out = np.zeros_like(self.x_pad)  # Gradient for the padded input\n",
    "    dw = np.zeros_like(self.w)          # Gradient for the filters (weights)\n",
    "    db = np.zeros_like(self.b)          # Gradient for the biases\n",
    "\n",
    "    # Compute bias gradient\n",
    "    # Hint: Sum over batch, height, and width dimensions of da, then divide by batch_size\n",
    "    db =  \n",
    "\n",
    "    # Compute gradients for weights and input\n",
    "    # Iterate over the height and width of the output\n",
    "    for i in range(h_out):\n",
    "        h_start = i * stride\n",
    "        h_end = h_start + h_f\n",
    "        for j in range(w_out):\n",
    "            w_start = j * stride\n",
    "            w_end = w_start + w_f\n",
    "\n",
    "            # Extract the gradient for the current output position, shape (batch_size, c_out, 1, 1, 1)\n",
    "            da_curr = da[:, :, np.newaxis, i:i+1, j:j+1]\n",
    "\n",
    "            # Extract the corresponding input slice, shape (batch_size, 1, c_in, h_f, w_f)\n",
    "            x_slice = self.x_pad[:, np.newaxis, :, h_start:h_end, w_start:w_end]\n",
    "\n",
    "            # Compute weight gradient\n",
    "            # Hint: Multiply da_curr and x_slice, then sum over the batch dimension\n",
    "            dw +=  \n",
    "\n",
    "            # Compute input gradient\n",
    "            # Hint: Multiply da_curr and self.w, then sum over the output channel dimension\n",
    "            out[:, :, h_start:h_end, w_start:w_end] +=  \n",
    "\n",
    "    # Normalize weight gradient by the batch size\n",
    "    dw /= batch_size\n",
    "\n",
    "    # Remove padding to get the gradient with respect to the original input\n",
    "    # Hint: Slice out to exclude the padding\n",
    "    out =  \n",
    "\n",
    "    # Update the stored gradients for weights and biases\n",
    "    self.dw += dw\n",
    "    self.db += db\n",
    "\n",
    "    # Return the gradient for the input\n",
    "    return out\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    The template class of network layers. You don't need to modify this.\n",
    "    \"\"\"\n",
    "    def get_params(self):\n",
    "        return\n",
    "\n",
    "    def set_params(self, w, b):\n",
    "        pass\n",
    "\n",
    "    def get_grads(self):\n",
    "        return\n",
    "\n",
    "    def zero_grad(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def backward(self, da):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`compute_output_dims` in `Conv2d`,  `backward` of `Conv2d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(Layer):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=(3, 3), stride=1, padding=(0, 0)):\n",
    "        \"\"\"\n",
    "        Similar to PyTorch Conv2d. Here bias=True, and padding_mode='zeros'.\n",
    "        \"\"\"\n",
    "        self.c_in = in_channels\n",
    "        self.c_out = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # init params\n",
    "        self.w = np.random.randn(out_channels, in_channels,\n",
    "                                 kernel_size[0], kernel_size[1]) * 0.1\n",
    "        self.b = np.random.randn(out_channels) * 0.1\n",
    "        self.dw = np.zeros_like(self.w)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "        self.x_pad = None\n",
    "        self.h_in = None\n",
    "        self.w_in = None\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.w, self.b\n",
    "\n",
    "    def set_params(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "\n",
    "    def get_grads(self):\n",
    "        return self.dw, self.db\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.dw = np.zeros_like(self.w)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_output_dims(h_in, w_in, h_f, w_f, h_p, w_p, stride):\n",
    "        \"\"\"\n",
    "        Compute the height and the width of the output tensor.\n",
    "        \"\"\"\n",
    "        # complete your code here\n",
    "\n",
    "        return h_out, w_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: 4D array (batch_size, channel, height, width)\n",
    "        \"\"\"\n",
    "        batch_size, c_in, h_in, w_in = x.shape\n",
    "        h_f, w_f = self.kernel_size\n",
    "        h_p, w_p = self.padding\n",
    "        h_out, w_out = self.compute_output_dims(\n",
    "            h_in, w_in, h_f, w_f, h_p, w_p, self.stride)\n",
    "\n",
    "        # padding\n",
    "        x_pad = np.pad(array=x, pad_width=(\n",
    "            (0, 0), (0, 0), (h_p, h_p), (w_p, w_p)))\n",
    "\n",
    "        # 2D convolution\n",
    "        out = np.zeros((batch_size, self.c_out, h_out, w_out))\n",
    "        for i in range(h_out):\n",
    "            h_start = i * self.stride\n",
    "            h_end = h_start + h_f\n",
    "            for j in range(w_out):\n",
    "                w_start = j * self.stride\n",
    "                w_end = w_start + w_f\n",
    "\n",
    "                out[:, :, i, j] = np.sum(\n",
    "                    self.w * x_pad[:, np.newaxis, :,\n",
    "                                   h_start:h_end, w_start:w_end],\n",
    "                    axis=(2, 3, 4)\n",
    "                )\n",
    "\n",
    "        # bias\n",
    "        out += self.b.repeat(h_out * w_out).reshape(-1, h_out, w_out)\n",
    "\n",
    "        # cache for backpropagation\n",
    "        self.x_pad = x_pad.copy()\n",
    "        self.h_in = h_in\n",
    "        self.w_in = w_in\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, da):\n",
    "        \"\"\"\n",
    "        da: 4D array (batch_size, channel, height, width)\n",
    "        \"\"\"\n",
    "        batch_size, c_out, h_out, w_out = da.shape\n",
    "        h_f, w_f = self.kernel_size\n",
    "        h_p, w_p = self.padding\n",
    "\n",
    "        db = np.zeros_like(self.b)\n",
    "        dw = np.zeros_like(self.w)  # (c_out, c_in, h_f, w_f)\n",
    "        out = np.zeros_like(self.x_pad)\n",
    "        \n",
    "        # complete your code here\n",
    "\n",
    "        self.dw += dw\n",
    "        self.db += db\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`compute_output_dims` in `MaxPool2d`, `forward` in `MaxPool2d`, `backward` in `MaxPool2d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2d(Layer):\n",
    "    def __init__(self, kernel_size=(3, 3), stride=1):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.h_in = None\n",
    "        self.w_in = None\n",
    "        self.max_indices = None\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_output_dims(h_in, w_in, h_f, w_f, stride):\n",
    "        \"\"\"\n",
    "        Compute the height and the width of the output tensor.\n",
    "        \"\"\"\n",
    "        # complete your code here\n",
    "\n",
    "        return h_out, w_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: 4D array (batch_size, channel, height, width)\n",
    "        \"\"\"\n",
    "        batch_size, c_in, h_in, w_in = x.shape\n",
    "        h_f, w_f = self.kernel_size\n",
    "        h_out, w_out = self.compute_output_dims(\n",
    "            h_in, w_in, h_f, w_f, self.stride)\n",
    "        \n",
    "        # cache for backpropagation\n",
    "        self.max_indices = np.zeros(\n",
    "            (batch_size, c_in, h_out, w_out), dtype=int)\n",
    "        self.h_in = h_in\n",
    "        self.w_in = w_in\n",
    "        \n",
    "        # max pooling\n",
    "        out = np.zeros((batch_size, c_in, h_out, w_out))\n",
    "        \n",
    "        # complete your code here\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, da):\n",
    "        \"\"\"\n",
    "        da: 4D array (batch_size, channel, height, width)\n",
    "        \"\"\"\n",
    "        batch_size, c_out, h_out, w_out = da.shape\n",
    "\n",
    "        out = np.zeros((batch_size, c_out, self.h_in, self.w_in))\n",
    "        \n",
    "        # complete your code here\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(Layer):\n",
    "    \"\"\"\n",
    "    Flatten the array to one dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.shape = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.shape = x.shape\n",
    "        out = x.reshape(x.shape[0], -1)\n",
    "        return out\n",
    "\n",
    "    def backward(self, da):\n",
    "        return da.reshape(self.shape)\n",
    "\n",
    "class FullyConnected(Layer):\n",
    "    \"\"\"\n",
    "    Similar to PyTorch Linear.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        # init params\n",
    "        self.w = np.random.randn(out_features, in_features) * 0.1\n",
    "        self.b = np.random.randn(out_features) * 0.1\n",
    "        self.dw = np.zeros_like(self.w)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        self.x = None\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.w, self.b\n",
    "\n",
    "    def set_params(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "\n",
    "    def get_grads(self):\n",
    "        return self.dw, self.db\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.dw = np.zeros_like(self.w)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: 2D array (batch_size, in_features)\n",
    "        \"\"\"\n",
    "        self.x = x.copy()\n",
    "        batch_size = x.shape[0]\n",
    "        out = np.zeros(shape=(batch_size, self.b.shape[0]))\n",
    "        for i in range(batch_size):\n",
    "            out[i, :] = np.dot(self.w, x[i, :]) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self, da):\n",
    "        \"\"\"\n",
    "        da: 2D array (batch_size, out_features)\n",
    "        \"\"\"\n",
    "        out = np.zeros(shape=(da.shape[0], self.w.shape[1]))\n",
    "        for i in range(out.shape[0]):\n",
    "            out[i, :] = np.dot(self.w.T, da[i, :])\n",
    "        dw = np.zeros_like(self.w)\n",
    "        for i in range(da.shape[0]):\n",
    "            dw += da[i][np.newaxis, :].T @ self.x[i][np.newaxis, :]\n",
    "        dw /= da.shape[0]\n",
    "        db = da.sum(0) / da.shape[0]\n",
    "        self.dw += dw\n",
    "        self.db += db\n",
    "        return out\n",
    "    \n",
    "class LeakyReLU(Layer):\n",
    "    \"\"\"\n",
    "    Similar to PyTorch LeakyReLU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, negative_slope=0.01):\n",
    "        self.negative_slope = negative_slope\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = x <= 0\n",
    "        x[self.mask] *= self.negative_slope\n",
    "        return x\n",
    "\n",
    "    def backward(self, da):\n",
    "        da[self.mask] *= self.negative_slope\n",
    "        return da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`forward` in `SoftMax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: 2D array (batch_size, class_num)\n",
    "        \"\"\"\n",
    "        # complete your code here\n",
    "        # remember to subtract max before np.exp() to avoid overflow.\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, da):\n",
    "        return da\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(Layer):\n",
    "    \"\"\"\n",
    "    Final sequential model.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers: List[Layer]):\n",
    "        self.layers = layers\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for layer in self.layers:\n",
    "            layer.zero_grad()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, x):\n",
    "        for layer in reversed(self.layers):\n",
    "            x = layer.backward(x)\n",
    "\n",
    "class AdamOptimizer:\n",
    "    def __init__(self, lr, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "\n",
    "        self.m = {}\n",
    "        self.v = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def clip_grad_norm(grad, max_norm=10):\n",
    "        clip_coef = max_norm / (np.linalg.norm(grad) + 1e-6)\n",
    "        if clip_coef < 1:\n",
    "            grad *= clip_coef\n",
    "        return grad\n",
    "\n",
    "    def update(self, layers: List[Layer]):\n",
    "        \"\"\"\n",
    "        update weights using gradients\n",
    "        \"\"\"\n",
    "        for idx, layer in enumerate(layers):\n",
    "            params = layer.get_params()\n",
    "            if params is not None:\n",
    "                w, b, = params\n",
    "                dw, db = layer.get_grads()\n",
    "\n",
    "                dw = self.clip_grad_norm(dw)\n",
    "                db = self.clip_grad_norm(db)\n",
    "\n",
    "                w_key = f'w{idx}'\n",
    "                b_key = f'b{idx}'\n",
    "\n",
    "                if w_key not in self.m:\n",
    "                    self.m[w_key] = np.zeros_like(w)\n",
    "                    self.v[w_key] = np.zeros_like(w)\n",
    "                    self.m[b_key] = np.zeros_like(b)\n",
    "                    self.v[b_key] = np.zeros_like(b)\n",
    "\n",
    "                self.m[w_key] = self.beta1 * \\\n",
    "                    self.m[w_key] + (1 - self.beta1) * dw\n",
    "                self.m[b_key] = self.beta1 * \\\n",
    "                    self.m[b_key] + (1 - self.beta1) * db\n",
    "\n",
    "                self.v[w_key] = self.beta2 * self.v[w_key] + \\\n",
    "                    (1 - self.beta2) * np.square(dw)\n",
    "                self.v[b_key] = self.beta2 * self.v[b_key] + \\\n",
    "                    (1 - self.beta2) * np.square(db)\n",
    "\n",
    "                dw = self.lr * self.m[w_key] / \\\n",
    "                    (np.sqrt(self.v[w_key]) + self.eps)\n",
    "                db = self.lr * self.m[b_key] / \\\n",
    "                    (np.sqrt(self.v[b_key]) + self.eps)\n",
    "\n",
    "                w -= dw\n",
    "                b -= db\n",
    "\n",
    "                layer.set_params(w, b)\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, x_train, y_train, batch_size):\n",
    "        \"\"\"\n",
    "        Similar to PyTorch DataLoader. Here shuffle=False, and drop last=True.\n",
    "        \"\"\"\n",
    "        assert len(x_train) == len(y_train) and batch_size <= len(x_train)\n",
    "        self.batch_size = batch_size\n",
    "        self.length = len(x_train) // batch_size\n",
    "        data_len = self.length * batch_size\n",
    "        self.x_train = np.split(x_train[:data_len], self.length)\n",
    "        self.y_train = np.split(y_train[:data_len], self.length)\n",
    "\n",
    "    def get_batch(self, idx):\n",
    "        \"\"\"\n",
    "        Gets a batch of data with index\n",
    "        \"\"\"\n",
    "        return self.x_train[idx], self.y_train[idx]\n",
    "\n",
    "def train(model: SequentialModel, epochs: int,\n",
    "          train_loader: DataLoader, optimizer: AdamOptimizer,\n",
    "          test_loader: DataLoader = None):\n",
    "    for epoch in range(epochs):\n",
    "        for idx in range(train_loader.length):\n",
    "            x_batch, y_batch = train_loader.get_batch(idx)\n",
    "            model.zero_grad()\n",
    "            out_batch = model.forward(x_batch)\n",
    "            da = out_batch - y_batch\n",
    "            model.backward(da)\n",
    "            optimizer.update(model.layers)\n",
    "        test(model, test_loader)\n",
    "    return model\n",
    "\n",
    "def test(model: SequentialModel, data_loader: DataLoader):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of predicted outputs on the test set.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for idx in range(data_loader.length):\n",
    "        x_batch, y_batch = data_loader.get_batch(idx)\n",
    "        out_batch = model.forward(x_batch)\n",
    "        pred_batch = np.argmax(out_batch, 1)\n",
    "        total += len(pred_batch)\n",
    "        correct += np.sum(pred_batch == y_batch)\n",
    "    acc = correct / total\n",
    "    print(acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in `main`, insert a `FullyConnected` model into pipeline with given parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    num_train, num_test = map(int, input().split())\n",
    "    h, w, num_classes = map(int, input().split())\n",
    "    seed = int(input())\n",
    "    # set random seed for initialization of weights and biases.\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    X_train = np.empty((num_train, h, w), dtype=float)\n",
    "    y_train = np.empty(num_train, dtype=int)\n",
    "    X_test = np.empty((num_test, h, w), dtype=float)\n",
    "    y_test = np.empty(num_test, dtype=int)\n",
    "    for b in range(num_train):\n",
    "        img = [input().split() for _ in range(h)]\n",
    "        X_train[b] = np.array(img, dtype=int) / 255\n",
    "        y_train[b] = int(input())\n",
    "    for b in range(num_test):\n",
    "        img = [input().split() for _ in range(h)]\n",
    "        X_test[b] = np.array(img, dtype=int) / 255\n",
    "        y_test[b] = int(input())\n",
    "\n",
    "    X_train = X_train[:, np.newaxis, :, :]\n",
    "    X_test = X_test[:, np.newaxis, :, :]\n",
    "    \n",
    "    # one-hot encoding for training.\n",
    "    one_hot_train = np.zeros((len(y_train), num_classes), dtype=float)\n",
    "    one_hot_train[np.arange(len(y_train)), y_train] = 1\n",
    "    \n",
    "    train_loader = DataLoader(X_train, one_hot_train, 200)\n",
    "    test_loader = DataLoader(X_test, y_test, 100)\n",
    "    \n",
    "    optimizer = AdamOptimizer(lr=0.02)\n",
    "    \n",
    "    model = SequentialModel([\n",
    "        Conv2d(1, 3, (3, 3), stride=1, padding=(1, 1)),\n",
    "        LeakyReLU(),\n",
    "        MaxPool2d((2, 2), 2),\n",
    "        Flatten(),\n",
    "        # add a fully connected layer here\n",
    "        # complete your code here\n",
    "        Softmax()\n",
    "    ])\n",
    "    \n",
    "    model = train(model, 5, train_loader, optimizer, test_loader)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objective</a></span></li><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#KMeans-theory\" data-toc-modified-id=\"KMeans-theory-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>KMeans theory</a></span></li><li><span><a href=\"#Choosing-K\" data-toc-modified-id=\"Choosing-K-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Choosing K</a></span><ul class=\"toc-item\"><li><span><a href=\"#Elbow-Method(手肘法)\" data-toc-modified-id=\"Elbow-Method(手肘法)-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Elbow Method(手肘法)</a></span></li><li><span><a href=\"#Silhouette-Analysis(轮廓系数法)\" data-toc-modified-id=\"Silhouette-Analysis(轮廓系数法)-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Silhouette Analysis(轮廓系数法)</a></span></li></ul></li><li><span><a href=\"#Advantages\" data-toc-modified-id=\"Advantages-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Advantages</a></span></li><li><span><a href=\"#Drawbacks\" data-toc-modified-id=\"Drawbacks-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Drawbacks</a></span></li><li><span><a href=\"#Other-resources\" data-toc-modified-id=\"Other-resources-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Other resources</a></span></li><li><span><a href=\"#LAB-Assignment\" data-toc-modified-id=\"LAB-Assignment-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>LAB Assignment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise\" data-toc-modified-id=\"Exercise-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Exercise</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-some-libraries\" data-toc-modified-id=\"Import-some-libraries-8.1.1\"><span class=\"toc-item-num\">8.1.1&nbsp;&nbsp;</span>Import some libraries</a></span></li><li><span><a href=\"#K-means\" data-toc-modified-id=\"K-means-8.1.2\"><span class=\"toc-item-num\">8.1.2&nbsp;&nbsp;</span>K-means</a></span></li><li><span><a href=\"#Load-video-and-detect\" data-toc-modified-id=\"Load-video-and-detect-8.1.3\"><span class=\"toc-item-num\">8.1.3&nbsp;&nbsp;</span>Load video and detect</a></span></li><li><span><a href=\"#Sample-Result\" data-toc-modified-id=\"Sample-Result-8.1.4\"><span class=\"toc-item-num\">8.1.4&nbsp;&nbsp;</span>Sample Result</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB11 tutorial for Machine Learning <br > Clustering with K-Means\n",
    "> The document description are designed by JIa Yanhong in 2022. Nov. 20th\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- Understand K-means algorithm theory\n",
    "- Implement the k-means algorithm  from scratch in python\n",
    "- Complete the LAB assignment and submit it to BB or sakai.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "**K-means** clustering is one of the simplest and popular **unsupervised** machine learning algorithms.\n",
    "\n",
    "Its goal is to separate the data into K distinct non-overlapping subgroups (clusters) of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares.\n",
    "\n",
    "$$\\sum_{i=0}^{n}\\min\\limits_{\\mu_j \\in C}\\left(||x_i - \\mu_j||^2\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans theory\n",
    "\n",
    "KMeans works as follows:\n",
    "\n",
    "1. First, pick the number of clusters (For more info, check the [\"Choosing K\" section](#choosing-k)).\n",
    "2. Initialize the center points of the cluster (centroids) by shuffling the dataset and then selecting K data points for the centroids.\n",
    "3. Assign data points to the cluster with the nearest centroid.\n",
    "4. Recompute centroid position by taking the mean of all data points assigned to the cluster. \n",
    "5. Repeat steps 3 and 4 for a set number of iterations or until the centroids aren't moving much between iterations anymore.\n",
    "<div  align=\"center\"> <img src=\"images/k_means.gif\"   width=600 align=center /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing K\n",
    "\n",
    "Choosing the right K value by hand can get quite tricky, especially if you're working with 3+ dimensional data. If you select a too small or big number for K, the result can be quite underwhelming.\n",
    "<div  align=\"center\"> <img src=\"images/choose_k_value.jpeg\"   width=600 align=center /></div>\n",
    "\n",
    "\n",
    "In this section, I'll show you two methods commonly used to choose the right K value:\n",
    "\n",
    "* The Elbow Method\n",
    "* Silhouette Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Method(手肘法)\n",
    "\n",
    "The Elbow Method shows us what a good number for K is based on the sum of squared distances (SSE) between data points and their assigned clusters' centroid. \n",
    "\n",
    "$$SSE=\\sum_{j=0}^{C}\\sum\\limits_{x_i \\in C_{j}}\\left(||x_i - \\mu_j||^2\\right)$$\n",
    "\n",
    "We pick k at the spot where the SSE starts to flatten out, which looks like an elbow. Below you can see an example created using [Yellowbrick](https://www.scikit-yb.org/en/latest/api/cluster/elbow.html).\n",
    "\n",
    "\n",
    "<div  align=\"center\"> <img src=\"images/elbow_method_using_yellowbrick.png\"   width=600 align=center /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Silhouette Analysis(轮廓系数法)\n",
    "\n",
    "The Silhouette Analysis can be used to study the separation distance between the resulting clusters. It displays a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation) and can thus be used to assess the number of clusters k. \n",
    "\n",
    "The Silhouette Analysis is computed as follows:\n",
    "\n",
    "* Compute the average distance between all data points in one cluster $C_i$\n",
    "\n",
    "$$a\\left(i\\right)=\\frac{1}{|C_i|-1}\\sum_{j\\in C_i,i\\neq j}d\\left(i,j\\right)$$\n",
    "\n",
    "* For all data points $i$ in cluster $C_i$ compute the average distance to all points in another cluster $C_k$ (where $C_k\\neq C_i$) \n",
    "\n",
    "$$b\\left(i\\right)=\\min\\limits_{k\\neq i}\\frac{1}{|C_k|}\\sum_{j\\in C_k}d\\left(i,j\\right)$$\n",
    "\n",
    ">The $min$ is used, because we want to know the average distance to the closed cluster $i$ is not a member of.\n",
    "\n",
    "With $a$ and $b$ we can now calculate the silhouette coefficient:\n",
    "\n",
    "$$s\\left(i\\right)=\\frac{b\\left(i\\right)-a\\left(i\\right)}{max\\{a\\left(i\\right),b\\left(i\\right)\\}}, if |C_i|>1$$\n",
    "\n",
    "The coefficient can take values in the interval $[-1, 1]$. Zero means the sample is very close to the neighboring clusters. One means the sample is far away from the neighboring cluster, and negative one means the sample is probably assigned to the wrong cluster.\n",
    "\n",
    "Below you can see an [example of silhouette analysis](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html) using [Scikit Learn](https://scikit-learn.org/stable/index.html):\n",
    "<div  align=\"center\"> <img src=\"images/silhouette_analysis_3_clusters.jpeg\"   width=600 align=center /></div>\n",
    "\n",
    "<div  align=\"center\"> <img src=\"images/silhouette_analysis_4_clusters.jpeg\"   width=600 align=center /></div>\n",
    "\n",
    "<div  align=\"center\"> <img src=\"images/silhouette_analysis_5_clusters.jpeg\"   width=600 align=center /></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages\n",
    "\n",
    "KMeans is an easy-to-implement algorithm that is also quite fast with an average complexity of $O(k*n*T)$, where n is the number of samples, and T is the number of iteration.\n",
    "\n",
    "## Drawbacks\n",
    "\n",
    "As mentioned above, KMeans makes use of the **sum-of-squares criterion**, which works well if the clusters have a spherical-like shape. It doesn't work well on many other types of data like complicated shapes, though. In this section, we'll go over a few cases where KMeans performs poorly.\n",
    "\n",
    "First, KMeans doesn't put data points that are far away from each other into the same cluster, even when they obviously should be because they underly some obvious structure like points on a line, for example.\n",
    "\n",
    "<div  align=\"center\"> <img src=\"images/two_lines.png\"   width=600 align=center /></div>\n",
    "\n",
    "\n",
    "In the image above, you can see that KMeans creates the clusters in between the two lines and therefore splits each line into one of two clusters rather than classifying each line as a cluster. On the right side, you can see the DBSCAN (Density-based spatial clustering of applications with noise) algorithm, which is able to separate the two lines without any issues.\n",
    "\n",
    "Also, KMeans performs poorly for complicated geometric shapes such as the moons and circles shown below.\n",
    "<div  align=\"center\"> <img src=\"images/noisy_moons_with_true_output.png\"   width=600 align=center /></div>\n",
    "\n",
    "<div  align=\"center\"> <img src=\"images/noisy_circles_with_true_output.png\"   width=600 align=center /></div>\n",
    "\n",
    "\n",
    "\n",
    "Other clustering algorithms like Spectral Clustering, Agglomerative Clustering, or DBSCAN don't have any problems with such data. For a more in-depth analysis of how different clustering algorithms perform on different interesting 2d datasets, I recommend checking out ['Comparing different clustering algorithms on toy datasets'](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html) from Scikit-Learn.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other resources\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/clustering.html#k-means\n",
    "* https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a\n",
    "* https://www.youtube.com/watch?v=4b5d3muPQmA\n",
    "* https://www.naftaliharris.com/blog/visualizing-k-means-clustering/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB Assignment\n",
    "\n",
    "In this lab, we will write a program to segment different objects in a video using *K-means* clustering. There are several steps:\n",
    "\n",
    "-  1.Load video & extract frames\n",
    "-  2.Implement *K-means* clustering\n",
    "-  3.Write back to video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tqdm\n",
    "import os\n",
    "import sys\n",
    "# color of different clusters\n",
    "GBR = [[0, 0, 255],\n",
    "       [0, 128, 255],\n",
    "       [255, 0, 0],\n",
    "       [128, 0, 128],\n",
    "       [255, 0, 255]]\n",
    "\n",
    "# path configuration\n",
    "project_root = os.path.abspath('.')\n",
    "output_path = os.path.join(project_root)\n",
    "input_path = os.path.join(project_root)\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### implement K-means\n",
    "In this lab, you need to implement K-means, the rough procedure is:\n",
    "\n",
    "1. **initialize centroids** of different classes\n",
    "\n",
    "   In the simplest case, randomly choose centroids in original data\n",
    "\n",
    "2. **calculate distances** between samples (pixels) and centroids\n",
    "\n",
    "   Since one sample (pixel) has 3 channels, you can calculate square sum of differences in each channel between it and centroids.\n",
    "   $$\n",
    "   dist(S,C) = \\sum_{i=1}^3(C_i-S_i)^2 \\\\\n",
    "   \\left\\{\n",
    "   \\begin{aligned}\n",
    "   &dist(S,C): \\text{distance between a sample S and a centroid C}\\\\\n",
    "   &C: \\text{a centroid}\\\\\n",
    "   &S: \\text{a sample}\\\\\n",
    "   &S_i: \\text{the } i^{th} \\text{ channel's value of S}\\\\\n",
    "   &C_i: \\text{the } i^{th} \\text{ channel's value of C}\n",
    "   \\end{aligned}\n",
    "   \\right.\n",
    "   $$\n",
    "   \n",
    "3. **classify** every samples\n",
    "\n",
    "   A sample is belonging to the class whose centroid is closest to it among all centroids.\n",
    "   $$\n",
    "   cls(S) = argmin(\\sum_{i=1}^3(C_i^k-S_i)^2), k=1,2,...,K\\\\\n",
    "   \\left\\{\n",
    "   \\begin{aligned}\n",
    "   &cls(S): \\text{class of a sample S}\\\\\n",
    "   &K: \\text{number of classes}\\\\\n",
    "   &C^k: \\text{centroid of } k^{th} \\text{ class}\\\\\n",
    "   \\end{aligned}\n",
    "   \\right.\n",
    "   $$\n",
    "\n",
    "4. **update centroid**\n",
    "\n",
    "   You can use mean of all samples in the same class to calculate new centroid.\n",
    "   $$\n",
    "   C^k_i =\\frac{1}{n^k}\\sum^{n^k}_{n=1}S^k_{in},\\ \\  i=1,2,3\\\\\n",
    "   \\left\\{\n",
    "   \\begin{aligned}\n",
    "   &C^k_i: \\text{the } i^{th} \\text{channel's value of a centroid belonging to the } k^{th} \\text{class} \\\\\n",
    "   &n^k: \\text{the number of samples in the }  k^{th} \\text{class}\\\\\n",
    "   &S^k_{in}: \\text{the } i^{th} \\text{channel's value of a sample which is in the } k^{th} \\text{class}\n",
    "   \\end{aligned}\n",
    "   \\right.\n",
    "   $$\n",
    "   \n",
    "5. loop until classification result doesn't change\n",
    "\n",
    "\n",
    "\n",
    "In addition, you may find there is code like this:\n",
    "\n",
    "```python\n",
    "while ret:\n",
    "    frame = np.float32(frame)\n",
    "    h, w, c = frame.shape\n",
    "    ...\n",
    "```\n",
    "\n",
    "Since if you don't converse the `dtype`, K-means hardly converges which means it will stuck into dead loop easily.\n",
    "\n",
    "\n",
    "\n",
    "After you finish K-means, you will find the written video is hard to watch because **color** between adjacent frames **changes almost all the time**. Here, I want you to find a way to alleviate the situation yourself.\n",
    "\n",
    "**It isn't compulsory**, you can try if you want.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(data: np.ndarray, n_cl: int):\n",
    "    \"\"\"\n",
    "        K-means\n",
    "\n",
    "    :param data:    original data\n",
    "    :param n_cl:    number of classes\n",
    "    :param seeds:   seeds\n",
    "    :return:        new labels and new seeds\n",
    "    \"\"\"\n",
    "    n_samples, channel = data.shape\n",
    "\n",
    "    # TODO: firstly you should init centroids by a certain strategy\n",
    "    centers = None\n",
    "\n",
    "    old_labels = np.zeros((n_samples,))\n",
    "    while True:\n",
    "        # TODO: calc distance between samples and centroids\n",
    "        distance = None\n",
    "        # TODO: classify samples\n",
    "        new_labels = old_labels\n",
    "\n",
    "        # TODO: update centroids\n",
    "        centers = centers\n",
    "\n",
    "        if np.all(new_labels == old_labels):            \n",
    "            break\n",
    "        old_labels = new_labels\n",
    "\n",
    "    return old_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load video and detect\n",
    "We use `opencv` to read a video.\n",
    "<font color=red>Pay attention</font> that data type of `frame` is `uint8`, not `int`; In this lab, frame has 3 channels.\n",
    "If you don't change `dtype` of frame into `unit8`, video you write will look strange which you can have a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin clustering with 1 classes:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:14<00:00,  2.35it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def detect(video, n_cl=2):\n",
    "    # load video, get number of frames and get shape of frame\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "            int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "\n",
    "    # instantiate a video writer\n",
    "    video_writer = cv2.VideoWriter(os.path.join(output_path, \"result_with_%dclz.mp4\" % n_cl),\n",
    "                                   cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                                   (fps / 10),\n",
    "                                   size,\n",
    "                                   isColor=True)\n",
    "\n",
    "    # initialize frame and seeds\n",
    "    ret, frame = cap.read()\n",
    " \n",
    "\n",
    "    print(\"Begin clustering with %d classes:\" % n_cl)\n",
    "    bar = tqdm.tqdm(total=fps)  # progress bar\n",
    "    while ret:\n",
    "        frame = np.float32(frame)\n",
    "        h, w, c = frame.shape\n",
    "\n",
    "        # k-means\n",
    "        data = frame.reshape((h * w, c))\n",
    "        labels = kmeans(data, n_cl=n_cl)\n",
    "\n",
    "        # give different cluster different colors\n",
    "        new_frame = np.zeros((h * w, c))\n",
    "        # TODO: dye pixels with colors\n",
    "        new_frame = new_frame.reshape((h, w, c)).astype(\"uint8\")\n",
    "        video_writer.write(new_frame)\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        bar.update()\n",
    "\n",
    "    # release resources\n",
    "    video_writer.release()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "video_sample = os.path.join(input_path, \"road_video.MOV\")\n",
    "detect(video_sample, n_cl=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Result\n",
    "<div  align=\"center\"> <img src=\"images/image-20220804142902993.png\"  alt=\"image-20220804142902993\" width=600 align=center /></div>\n",
    "\n",
    "<div  align=\"center\"> <img src=\"images/image-20220804143125976.png\"   width=600 align=center /></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "773bfaa0e82744962f3138a2d7b2f007250d49f330da41a556809ccbcf17bfbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

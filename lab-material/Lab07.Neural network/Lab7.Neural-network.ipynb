{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB7 tutorial for Machine Learning <br >Neural NetWork & Pytorch\n",
    "> The document description are designed by JIa Yanhong in 2022. Oct. 20th\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- Install the pytorch neural network framework for your computer.\n",
    "- Learn to use pytorch.\n",
    "- Implement a simple neural network using pytorch\n",
    "- Complete the LAB assignment.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "Neural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of  machine learning and are at the heart of  deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another.\n",
    "\n",
    "![img](images/1hkYlTODpjJgo32DoCOWN5w.png)\n",
    "\n",
    "The above neural could be represented as:\n",
    "$$y=f(b+\\sum_{i=1}^{n}w_ix_i)=f(w^Tx)$$\n",
    "\n",
    "Artificial neural networks (ANNs) are comprised of a node layers, containing `an input layer`, `one or more hidden layers`, and `an output layer`. Each node, or artificial neuron, connects to another and has an associated `weight` and `threshold`. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.\n",
    "\n",
    "<img src=\"images/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork-WHITEBG.png\" alt=\"Visual diagram of an input layer, hidden layers, and an output layer of a feedforward neural network \" style=\"zoom:40%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The role of neural networks in Machine learning\n",
    "+ Supervise machine learning process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Supervise machine learning process ](images/Supervise-machine-learning-process.png)\n",
    "\n",
    "  In the figure above, the hardest part is how to obtain valid feature vectors. This is a technique called **feature engineering**.\n",
    "\n",
    "+ **The Importance of Feature Engineering:**\n",
    "\n",
    "  + Preprocessing and feature extraction determine the upper bound of the model\n",
    "  + The algorithm and parameter selection approach this upper bound.\n",
    "\n",
    "+ **Traditional feature extraction methods:**\n",
    "\n",
    "  <img src=\"images/image-20221020212446504.png\" alt=\"image-20221020212446504 \" style=\"zoom:60%;\" />\n",
    "\n",
    "+ **Neural networks automatically extract features**\n",
    "  \n",
    "  <img src=\"images/image-20221020212805853.png\" alt=\"image-20221020212805853 \" style=\"zoom:60%;\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch\n",
    "### Install Pytorch\n",
    "\n",
    "First check your CUDA version installed on your system using Nvidia control panel:\n",
    "\n",
    "<img src=\"images/check_cuda.png\" alt=\"check_cuda\" style=\"zoom:60%;\" />\n",
    "\n",
    "Upgrade your Nvidia driver if your CUDA version is low.\n",
    "\n",
    "Then follow the steps to install Pytorch:\n",
    "\n",
    "https://pytorch.org/get-started/locally/\n",
    "\n",
    "If your computer has no CUDA, don't panic, there are other ways too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning pytorch with linear regression\n",
    "#### Linear regression\n",
    "Do you remember the general form of linear regression model's prediction?\n",
    "$$\\hat{y}=h_{\\theta }(x)=\\theta _{0}+\\theta _{1}x_{1}+\\theta _{2}x_{2}+...+\\theta _{n}x_{n}=\\theta ^{T}\\cdot x$$\n",
    "\n",
    "<center>\n",
    "    <img src='images/fit-linreg.svg' style=\"zoom:100%;\"/>\n",
    "    <br>\n",
    "    <div style=\"\">\n",
    "       Fitting a linear regression model to one-dimensional data\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "Linear regression is a single-layer neural network, We used this simple network to learn how to use pytorch.\n",
    "<center>\n",
    "    <img src='images/singleneuron.svg' style=\"zoom:100%;\"/>\n",
    "    <br>\n",
    "    <div style=\"\">\n",
    "        Linear regression is a single-layer neural network \n",
    "    </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warm-up: [numpy](https://numpy.org/learn/)\n",
    "Before introducing PyTorch, we will first implement the network using numpy.\n",
    "\n",
    "Numpy provides an n-dimensional array object, and many functions for manipulating these arrays. Numpy is a generic framework for scientific computing; it does not know anything about computation graphs, or deep learning, or gradients. However we can easily use numpy to fit a third order polynomial to sine function by manually implementing the forward and backward passes through the network using numpy operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1afe867e0f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# Create random input and output data\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "plt.scatter(x,y,c = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1385.7625138811393\n",
      "199 976.2863527450745\n",
      "299 688.8676390480605\n",
      "399 487.02714555836366\n",
      "499 345.22050115725204\n",
      "599 245.5492875022948\n",
      "699 175.4654235865668\n",
      "799 126.16710863491423\n",
      "899 91.47722344569573\n",
      "999 67.05856585872009\n",
      "1099 49.8644200419963\n",
      "1199 37.753658308988456\n",
      "1299 29.22095096619963\n",
      "1399 23.207557279767265\n",
      "1499 18.968561046038296\n",
      "1599 15.979666585234582\n",
      "1699 13.871736684142562\n",
      "1799 12.384794850540478\n",
      "1899 11.335690880186768\n",
      "1999 10.595362131369786\n",
      "Result: y = -0.043904824978128754 + 0.8493561986701685 x + 0.007574312888168338 x^2 + -0.09227998768023372 x^3\n"
     ]
    }
   ],
   "source": [
    "# Randomly initialize weights\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch: Tensors \n",
    "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won’t be enough for modern deep learning.\n",
    "\n",
    "Here we introduce the most fundamental PyTorch concept: the Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadro P4000\n",
      "99 568.5546875\n",
      "199 404.85906982421875\n",
      "299 289.0391845703125\n",
      "399 207.09190368652344\n",
      "499 149.11026000976562\n",
      "599 108.08495330810547\n",
      "699 79.05703735351562\n",
      "799 58.517791748046875\n",
      "899 43.984779357910156\n",
      "999 33.701541900634766\n",
      "1099 26.425289154052734\n",
      "1199 21.276691436767578\n",
      "1299 17.63359260559082\n",
      "1399 15.055734634399414\n",
      "1499 13.231651306152344\n",
      "1599 11.940919876098633\n",
      "1699 11.027585983276367\n",
      "1799 10.38129997253418\n",
      "1899 9.923981666564941\n",
      "1999 9.60037612915039\n",
      "Result: y = 0.029613465070724487 + 0.8560596704483032 x + -0.005108816083520651 x^2 + -0.09323348850011826 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device = torch.device(device)\n",
    "print(torch.cuda.get_device_name(device))\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch: Tensors and autograd\n",
    "In the above examples, we had to manually implement both the forward and backward passes of our neural network. Manually implementing the backward pass is not a big deal for a small two-layer network, but can quickly get very hairy for large complex networks.\n",
    "\n",
    "Thankfully, we can use automatic differentiation to automate the computation of backward passes in neural networks. The autograd package in PyTorch provides exactly this functionality. \n",
    "\n",
    "When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients.\n",
    "\n",
    "This sounds complicated, it’s pretty simple to use in practice. Each Tensor represents a node in a computational graph. If `x` is a Tensor that has `x.requires_grad=True` then `x.grad` is another Tensor holding the gradient of `x` with respect to some scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1661.2891845703125\n",
      "199 1146.617919921875\n",
      "299 793.10888671875\n",
      "399 550.018798828125\n",
      "499 382.67205810546875\n",
      "599 267.3403625488281\n",
      "699 187.77066040039062\n",
      "799 132.81483459472656\n",
      "899 94.81932067871094\n",
      "999 68.52279663085938\n",
      "1099 50.30497741699219\n",
      "1199 37.671600341796875\n",
      "1299 28.902402877807617\n",
      "1399 22.80990982055664\n",
      "1499 18.57327651977539\n",
      "1599 15.624648094177246\n",
      "1699 13.570720672607422\n",
      "1799 12.138849258422852\n",
      "1899 11.139869689941406\n",
      "1999 10.442389488220215\n",
      "Result: y = 0.03906906023621559 + 0.8409804105758667 x + -0.006740061566233635 x^2 + -0.09108860045671463 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.data.item())\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    # grad_y_pred = 2.0 * (y_pred - y)\n",
    "    # grad_a = grad_y_pred.sum()\n",
    "    # grad_b = (grad_y_pred * x).sum()\n",
    "    # grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    # grad_d = (grad_y_pred * x ** 3).sum()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a.data -= learning_rate * a.grad.data\n",
    "    b.data -= learning_rate * b.grad.data\n",
    "    c.data -= learning_rate * c.grad.data\n",
    "    d.data -= learning_rate * d.grad.data\n",
    "\n",
    "    a.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "    c.grad.data.zero_()\n",
    "    d.grad.data.zero_()\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.data.item()} + {b.data.item()} x + {c.data.item()} x^2 + {d.data.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch: [nn module](https://pytorch.org/docs/stable/nn.html)\n",
    "we use the nn package to implement our polynomial model network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Using torch.nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 526.9667358398438\n",
      "199 353.1650695800781\n",
      "299 237.73545837402344\n",
      "399 161.05149841308594\n",
      "499 110.0920639038086\n",
      "599 76.21690368652344\n",
      "699 53.69078826904297\n",
      "799 38.70606231689453\n",
      "899 28.734182357788086\n",
      "999 22.095535278320312\n",
      "1099 17.674022674560547\n",
      "1199 14.72800064086914\n",
      "1299 12.764058113098145\n",
      "1399 11.45417594909668\n",
      "1499 10.580093383789062\n",
      "1599 9.996491432189941\n",
      "1699 9.606626510620117\n",
      "1799 9.346004486083984\n",
      "1899 9.171680450439453\n",
      "1999 9.054990768432617\n",
      "Result: y = -0.007631894201040268 + 0.8434891700744629 x + 0.0013166292337700725 x^2 + -0.0914454534649849 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3) \n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. The Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
    "# to match the shape of `y`.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Define the class\n",
    "  \n",
    "We define our neural network by subclassing `nn.Module`, and initialize the neural network layers in __init__. Every `nn.Module` subclass implements the operations on input data in the `forward()` method.\n",
    "The `forward()` method is in charge of conducting the **forward propagation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()       \n",
    "        self.linear = nn.Linear(3, 1)\n",
    "        self.flatten = nn.Flatten(0, 1)\n",
    "#       self.model = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(3, 1),\n",
    "#     torch.nn.Flatten(0, 1)\n",
    "# )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.flatten(self.linear(x))\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 929.337646484375\n",
      "199 627.2691040039062\n",
      "299 424.673828125\n",
      "399 288.6889343261719\n",
      "499 197.34034729003906\n",
      "599 135.9253387451172\n",
      "699 94.59913635253906\n",
      "799 66.7658920288086\n",
      "899 48.00289535522461\n",
      "999 35.342411041259766\n",
      "1099 26.79125213623047\n",
      "1199 21.009859085083008\n",
      "1299 17.097068786621094\n",
      "1399 14.446147918701172\n",
      "1499 12.648221969604492\n",
      "1599 11.427528381347656\n",
      "1699 10.597798347473145\n",
      "1799 10.033182144165039\n",
      "1899 9.648568153381348\n",
      "1999 9.386255264282227\n",
      "Result: y = -0.018171029165387154 + 0.8406386971473694 x + 0.003134804544970393 x^2 + -0.0910400003194809 x^3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3) \n",
    "\n",
    "\n",
    "model = LinearModel()\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "for t in range(2000):\n",
    "\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model.linear\n",
    "\n",
    "\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch: optim\n",
    "Up to this point we have updated the weights of our models by manually mutating the Tensors holding learnable parameters with `torch.no_grad()`. This is not a huge burden for simple optimization algorithms like stochastic gradient descent, but in practice we often train neural networks using more sophisticated optimizers like AdaGrad, RMSProp, Adam, etc.\n",
    "\n",
    "The `optim` package in PyTorch abstracts the idea of an optimization algorithm and provides implementations of commonly used optimization algorithms.\n",
    "\n",
    "In this example we will use the `nn` package to define our model as before, but we will optimize the model using the `RMSprop` algorithm provided by the `optim` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 451.5672912597656\n",
      "199 307.3272705078125\n",
      "299 210.72842407226562\n",
      "399 139.80429077148438\n",
      "499 87.41068267822266\n",
      "599 50.73720169067383\n",
      "699 27.414106369018555\n",
      "799 15.329279899597168\n",
      "899 10.292642593383789\n",
      "999 9.068931579589844\n",
      "1099 8.899371147155762\n",
      "1199 8.89703369140625\n",
      "1299 8.924613952636719\n",
      "1399 8.9288969039917\n",
      "1499 8.918439865112305\n",
      "1599 8.91790771484375\n",
      "1699 8.921748161315918\n",
      "1799 8.921767234802246\n",
      "1899 8.920357704162598\n",
      "1999 8.920419692993164\n",
      "Result: y = 0.0004998068907298148 + 0.8562415838241577 x + 0.0004998052609153092 x^2 + -0.09382955729961395 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3) \n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. The Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
    "# to match the shape of `y`.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "model.requires_grad_()\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "optimizer = torch.optim.RMSprop(params=model.parameters(),lr=0.001)\n",
    "#optimizer = torch.optim.SGD(model.parameters(),lr=1e-6,momentum=0.9)\n",
    "\n",
    "for t in range(2000):\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass: compute predicted y by passing x to the model. \n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "   \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())    \n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. \n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning pytorch with logistic regression\n",
    "If we use a single-layer network for classification, this is known as a logistic regression.\n",
    "\n",
    "\n",
    " We need to add the sigmoid function to the output of the linear regression.\n",
    "<center>\n",
    "    <img src='images/Center.png' style=\"zoom:100%;\"/>\n",
    "    <br>\n",
    "    <div style=\"\">\n",
    "       Perceptron\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "Let us define the number of epochs and the learning rate we want our model for training. As the data is a binary  classification, we will use **Binary Cross Entropy** as the **loss function** used to optimize the model using an `SGD optimizer`.\n",
    "\n",
    "<font size=5 color='red'>Please complete this part of the code!!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "### Perceptron-The basic unit of neural network\n",
    "A simple model of a biological neuron in an artificial neural network is known as Perceptron. it is the primary step to learn Machine Learning and Deep Learning technologies.\n",
    "\n",
    "we can consider it as a single-layer neural network with four main parameters, i.e., `input values`, `weights and Bias`, `net sum`, and an `activation function`.\n",
    "\n",
    "![Perceptron in Machine Learning](images/perceptron-in-machine-learning2.png)\n",
    "\n",
    "- **Input Nodes or Input Layer:**\n",
    "\n",
    "This is the primary component of Perceptron which accepts the initial data into the system for further processing.\n",
    "\n",
    "- **Wight and Bias:**\n",
    "\n",
    "Weight parameter represents the strength of the connection between units.  Bias can be considered as the intercept in a linear equation.\n",
    "\n",
    "- **Activation Function:**\n",
    "\n",
    "These are the final and important components that help to determine whether the neuron will fire or not. The activation function of perceptron is `sign function`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron(MLP)\n",
    "A neuron is a mathematical model of the behaviour of a single neuron in a biological nervous system.\n",
    "\n",
    "A single neuron can solve some simple tasks, but the power of neural networks comes when many of them are arranged in layers and connected in a network architecture.\n",
    "\n",
    "<img src=\"images/multilayer-perceptron-1.png\" alt=\"multilayer-perceptron-1 \" style=\"zoom:40%;\" />\n",
    "\n",
    "\n",
    "**A Multilayered Perceptron is a Neural Network**. A neural network having more than 3 hidden layers is called a **Deep Neural Network**.\n",
    "\n",
    "In this lab, Multilayer Perceptron and Neural Network  mean the same thing.\n",
    "\n",
    "------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "Various activation functions that can be used with Perceptron.\n",
    "\n",
    "![Perceptron_36.](images/Perceptron_36.jpg)\n",
    "\n",
    "<font color=\"red\">Neural network without activation functions are simply linear regression model</font>. The activation makes the input capable of learning and performing more complex tasks.\n",
    "\n",
    "![image-20221023014216170](images/image-20221023014216170.png)\n",
    "\n",
    "Therefore, when we write the neural network framework, the neurons in each hidden layer are most of the time **followed by an activation function**.\n",
    "\n",
    "I recommend that you use the relu function as you build your neural network framework.\n",
    "\n",
    "![image-20221023015025204](images/image-20221023015025204.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for MLP\n",
    "This Example uses dataset digit123.csv , which has 36 columns, and the last column is the dependent variable. We use this dataset to familiarize ourselves with MLP and solve the multi-classification problem.\n",
    "\n",
    "\n",
    "**Note that the values of the dependent variable are 1,2,3, and label coding is required.**\n",
    "#### MLP Model \n",
    "\n",
    "+ step 1 load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  27  28  29  30  31  32  33  \\\n",
       "0   0   0   0   1   0   0   0   0   0   1  ...   1   0   0   0   0   0   1   \n",
       "1   0   0   0   1   0   0   0   0   1   1  ...   1   0   0   0   0   0   1   \n",
       "2   0   0   1   1   0   0   0   0   0   1  ...   1   0   0   0   0   0   1   \n",
       "3   0   0   0   1   0   0   0   0   0   1  ...   1   0   0   0   0   1   1   \n",
       "4   0   0   0   1   0   0   0   0   0   1  ...   1   0   0   0   0   1   1   \n",
       "\n",
       "   34  35  36  \n",
       "0   0   0   1  \n",
       "1   0   0   1  \n",
       "2   0   0   1  \n",
       "3   0   0   1  \n",
       "4   1   0   1  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 'Matplotlib' is a data visualization library for 2D and 3D plots, built on numpy\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# to suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "# ============================ step 1/6 load datasets ============================\n",
    "df = pd.read_csv(\"datasets/digit123.csv\", header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36\n",
       "1    32\n",
       "2    32\n",
       "3    32\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[36].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 36)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df[36]\n",
    "y.replace((1, 2, 3),(0, 1, 2),inplace=True)\n",
    "X = df.drop(36, axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting for X and Y variables:\n",
    "from sklearn.model_selection import train_test_split\n",
    "## Splitting dataset into 80% Training and 20% Testing Data:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.8, random_state =0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch \n",
    "#Converting them to tensors as PyTorch works on, we will use the torch.from_numpy() method:\n",
    "X_train = torch.from_numpy(X_train.values).float()\n",
    "X_test = torch.from_numpy(X_test.values).float()\n",
    "y_train = torch.from_numpy(y_train.values).long()\n",
    "y_test = torch.from_numpy(y_test.values).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 2 Define a MLP subclass of nn. Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a MLP subclass of nn. Module.\n",
    "# ============================ step 2/6 define model ============================\n",
    "import  torch \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_i, n_h, n_o):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(n_i, n_h)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(n_h, n_o)\n",
    "    def forward(self, input):\n",
    "        return self.linear2(self.relu(self.linear1(self.flatten(input))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 3 Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 10000\n",
    "learning_rate = 0.001 \n",
    "# Create the model\n",
    "# ============================ step 3/6 Create model ============================\n",
    "models = MLP(X_train.shape[1],X_train.shape[1]//2,y_train.unique().size()[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 4 Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ step 4/6 Loss function ============================\n",
    "criterions = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 5 The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ step 4/6 The optimizer ============================\n",
    "optimizers = torch.optim.SGD(models.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 6 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, loss = 1.1067\n",
      "epoch: 40, loss = 1.1046\n",
      "epoch: 60, loss = 1.1026\n",
      "epoch: 80, loss = 1.1005\n",
      "epoch: 100, loss = 1.0985\n",
      "epoch: 120, loss = 1.0966\n",
      "epoch: 140, loss = 1.0946\n",
      "epoch: 160, loss = 1.0927\n",
      "epoch: 180, loss = 1.0908\n",
      "epoch: 200, loss = 1.0890\n",
      "epoch: 220, loss = 1.0871\n",
      "epoch: 240, loss = 1.0853\n",
      "epoch: 260, loss = 1.0835\n",
      "epoch: 280, loss = 1.0817\n",
      "epoch: 300, loss = 1.0799\n",
      "epoch: 320, loss = 1.0781\n",
      "epoch: 340, loss = 1.0763\n",
      "epoch: 360, loss = 1.0745\n",
      "epoch: 380, loss = 1.0727\n",
      "epoch: 400, loss = 1.0710\n",
      "epoch: 420, loss = 1.0692\n",
      "epoch: 440, loss = 1.0674\n",
      "epoch: 460, loss = 1.0657\n",
      "epoch: 480, loss = 1.0640\n",
      "epoch: 500, loss = 1.0623\n",
      "epoch: 520, loss = 1.0606\n",
      "epoch: 540, loss = 1.0588\n",
      "epoch: 560, loss = 1.0571\n",
      "epoch: 580, loss = 1.0553\n",
      "epoch: 600, loss = 1.0535\n",
      "epoch: 620, loss = 1.0517\n",
      "epoch: 640, loss = 1.0499\n",
      "epoch: 660, loss = 1.0480\n",
      "epoch: 680, loss = 1.0461\n",
      "epoch: 700, loss = 1.0441\n",
      "epoch: 720, loss = 1.0422\n",
      "epoch: 740, loss = 1.0402\n",
      "epoch: 760, loss = 1.0381\n",
      "epoch: 780, loss = 1.0360\n",
      "epoch: 800, loss = 1.0339\n",
      "epoch: 820, loss = 1.0317\n",
      "epoch: 840, loss = 1.0294\n",
      "epoch: 860, loss = 1.0271\n",
      "epoch: 880, loss = 1.0248\n",
      "epoch: 900, loss = 1.0225\n",
      "epoch: 920, loss = 1.0201\n",
      "epoch: 940, loss = 1.0177\n",
      "epoch: 960, loss = 1.0152\n",
      "epoch: 980, loss = 1.0128\n",
      "epoch: 1000, loss = 1.0103\n",
      "epoch: 1020, loss = 1.0078\n",
      "epoch: 1040, loss = 1.0053\n",
      "epoch: 1060, loss = 1.0028\n",
      "epoch: 1080, loss = 1.0003\n",
      "epoch: 1100, loss = 0.9977\n",
      "epoch: 1120, loss = 0.9951\n",
      "epoch: 1140, loss = 0.9925\n",
      "epoch: 1160, loss = 0.9898\n",
      "epoch: 1180, loss = 0.9870\n",
      "epoch: 1200, loss = 0.9843\n",
      "epoch: 1220, loss = 0.9816\n",
      "epoch: 1240, loss = 0.9788\n",
      "epoch: 1260, loss = 0.9759\n",
      "epoch: 1280, loss = 0.9731\n",
      "epoch: 1300, loss = 0.9702\n",
      "epoch: 1320, loss = 0.9672\n",
      "epoch: 1340, loss = 0.9643\n",
      "epoch: 1360, loss = 0.9613\n",
      "epoch: 1380, loss = 0.9584\n",
      "epoch: 1400, loss = 0.9554\n",
      "epoch: 1420, loss = 0.9523\n",
      "epoch: 1440, loss = 0.9493\n",
      "epoch: 1460, loss = 0.9462\n",
      "epoch: 1480, loss = 0.9430\n",
      "epoch: 1500, loss = 0.9398\n",
      "epoch: 1520, loss = 0.9366\n",
      "epoch: 1540, loss = 0.9334\n",
      "epoch: 1560, loss = 0.9302\n",
      "epoch: 1580, loss = 0.9269\n",
      "epoch: 1600, loss = 0.9235\n",
      "epoch: 1620, loss = 0.9202\n",
      "epoch: 1640, loss = 0.9168\n",
      "epoch: 1660, loss = 0.9134\n",
      "epoch: 1680, loss = 0.9099\n",
      "epoch: 1700, loss = 0.9064\n",
      "epoch: 1720, loss = 0.9030\n",
      "epoch: 1740, loss = 0.8995\n",
      "epoch: 1760, loss = 0.8960\n",
      "epoch: 1780, loss = 0.8924\n",
      "epoch: 1800, loss = 0.8889\n",
      "epoch: 1820, loss = 0.8853\n",
      "epoch: 1840, loss = 0.8816\n",
      "epoch: 1860, loss = 0.8780\n",
      "epoch: 1880, loss = 0.8743\n",
      "epoch: 1900, loss = 0.8706\n",
      "epoch: 1920, loss = 0.8669\n",
      "epoch: 1940, loss = 0.8631\n",
      "epoch: 1960, loss = 0.8593\n",
      "epoch: 1980, loss = 0.8555\n",
      "epoch: 2000, loss = 0.8516\n",
      "epoch: 2020, loss = 0.8477\n",
      "epoch: 2040, loss = 0.8438\n",
      "epoch: 2060, loss = 0.8399\n",
      "epoch: 2080, loss = 0.8359\n",
      "epoch: 2100, loss = 0.8319\n",
      "epoch: 2120, loss = 0.8279\n",
      "epoch: 2140, loss = 0.8238\n",
      "epoch: 2160, loss = 0.8198\n",
      "epoch: 2180, loss = 0.8157\n",
      "epoch: 2200, loss = 0.8115\n",
      "epoch: 2220, loss = 0.8074\n",
      "epoch: 2240, loss = 0.8032\n",
      "epoch: 2260, loss = 0.7990\n",
      "epoch: 2280, loss = 0.7948\n",
      "epoch: 2300, loss = 0.7906\n",
      "epoch: 2320, loss = 0.7863\n",
      "epoch: 2340, loss = 0.7821\n",
      "epoch: 2360, loss = 0.7778\n",
      "epoch: 2380, loss = 0.7735\n",
      "epoch: 2400, loss = 0.7691\n",
      "epoch: 2420, loss = 0.7648\n",
      "epoch: 2440, loss = 0.7604\n",
      "epoch: 2460, loss = 0.7560\n",
      "epoch: 2480, loss = 0.7516\n",
      "epoch: 2500, loss = 0.7472\n",
      "epoch: 2520, loss = 0.7428\n",
      "epoch: 2540, loss = 0.7384\n",
      "epoch: 2560, loss = 0.7339\n",
      "epoch: 2580, loss = 0.7294\n",
      "epoch: 2600, loss = 0.7249\n",
      "epoch: 2620, loss = 0.7204\n",
      "epoch: 2640, loss = 0.7159\n",
      "epoch: 2660, loss = 0.7114\n",
      "epoch: 2680, loss = 0.7069\n",
      "epoch: 2700, loss = 0.7024\n",
      "epoch: 2720, loss = 0.6978\n",
      "epoch: 2740, loss = 0.6933\n",
      "epoch: 2760, loss = 0.6887\n",
      "epoch: 2780, loss = 0.6841\n",
      "epoch: 2800, loss = 0.6796\n",
      "epoch: 2820, loss = 0.6750\n",
      "epoch: 2840, loss = 0.6704\n",
      "epoch: 2860, loss = 0.6658\n",
      "epoch: 2880, loss = 0.6612\n",
      "epoch: 2900, loss = 0.6566\n",
      "epoch: 2920, loss = 0.6521\n",
      "epoch: 2940, loss = 0.6475\n",
      "epoch: 2960, loss = 0.6429\n",
      "epoch: 2980, loss = 0.6384\n",
      "epoch: 3000, loss = 0.6338\n",
      "epoch: 3020, loss = 0.6293\n",
      "epoch: 3040, loss = 0.6247\n",
      "epoch: 3060, loss = 0.6202\n",
      "epoch: 3080, loss = 0.6157\n",
      "epoch: 3100, loss = 0.6112\n",
      "epoch: 3120, loss = 0.6067\n",
      "epoch: 3140, loss = 0.6022\n",
      "epoch: 3160, loss = 0.5977\n",
      "epoch: 3180, loss = 0.5933\n",
      "epoch: 3200, loss = 0.5888\n",
      "epoch: 3220, loss = 0.5844\n",
      "epoch: 3240, loss = 0.5800\n",
      "epoch: 3260, loss = 0.5756\n",
      "epoch: 3280, loss = 0.5712\n",
      "epoch: 3300, loss = 0.5668\n",
      "epoch: 3320, loss = 0.5625\n",
      "epoch: 3340, loss = 0.5581\n",
      "epoch: 3360, loss = 0.5538\n",
      "epoch: 3380, loss = 0.5495\n",
      "epoch: 3400, loss = 0.5452\n",
      "epoch: 3420, loss = 0.5410\n",
      "epoch: 3440, loss = 0.5367\n",
      "epoch: 3460, loss = 0.5325\n",
      "epoch: 3480, loss = 0.5283\n",
      "epoch: 3500, loss = 0.5241\n",
      "epoch: 3520, loss = 0.5199\n",
      "epoch: 3540, loss = 0.5158\n",
      "epoch: 3560, loss = 0.5117\n",
      "epoch: 3580, loss = 0.5076\n",
      "epoch: 3600, loss = 0.5035\n",
      "epoch: 3620, loss = 0.4995\n",
      "epoch: 3640, loss = 0.4955\n",
      "epoch: 3660, loss = 0.4915\n",
      "epoch: 3680, loss = 0.4875\n",
      "epoch: 3700, loss = 0.4836\n",
      "epoch: 3720, loss = 0.4797\n",
      "epoch: 3740, loss = 0.4758\n",
      "epoch: 3760, loss = 0.4719\n",
      "epoch: 3780, loss = 0.4680\n",
      "epoch: 3800, loss = 0.4642\n",
      "epoch: 3820, loss = 0.4604\n",
      "epoch: 3840, loss = 0.4567\n",
      "epoch: 3860, loss = 0.4529\n",
      "epoch: 3880, loss = 0.4492\n",
      "epoch: 3900, loss = 0.4455\n",
      "epoch: 3920, loss = 0.4419\n",
      "epoch: 3940, loss = 0.4383\n",
      "epoch: 3960, loss = 0.4347\n",
      "epoch: 3980, loss = 0.4311\n",
      "epoch: 4000, loss = 0.4276\n",
      "epoch: 4020, loss = 0.4241\n",
      "epoch: 4040, loss = 0.4206\n",
      "epoch: 4060, loss = 0.4172\n",
      "epoch: 4080, loss = 0.4138\n",
      "epoch: 4100, loss = 0.4104\n",
      "epoch: 4120, loss = 0.4070\n",
      "epoch: 4140, loss = 0.4037\n",
      "epoch: 4160, loss = 0.4004\n",
      "epoch: 4180, loss = 0.3972\n",
      "epoch: 4200, loss = 0.3939\n",
      "epoch: 4220, loss = 0.3907\n",
      "epoch: 4240, loss = 0.3875\n",
      "epoch: 4260, loss = 0.3844\n",
      "epoch: 4280, loss = 0.3812\n",
      "epoch: 4300, loss = 0.3781\n",
      "epoch: 4320, loss = 0.3751\n",
      "epoch: 4340, loss = 0.3720\n",
      "epoch: 4360, loss = 0.3690\n",
      "epoch: 4380, loss = 0.3660\n",
      "epoch: 4400, loss = 0.3630\n",
      "epoch: 4420, loss = 0.3601\n",
      "epoch: 4440, loss = 0.3572\n",
      "epoch: 4460, loss = 0.3543\n",
      "epoch: 4480, loss = 0.3515\n",
      "epoch: 4500, loss = 0.3486\n",
      "epoch: 4520, loss = 0.3458\n",
      "epoch: 4540, loss = 0.3431\n",
      "epoch: 4560, loss = 0.3403\n",
      "epoch: 4580, loss = 0.3376\n",
      "epoch: 4600, loss = 0.3349\n",
      "epoch: 4620, loss = 0.3323\n",
      "epoch: 4640, loss = 0.3297\n",
      "epoch: 4660, loss = 0.3271\n",
      "epoch: 4680, loss = 0.3245\n",
      "epoch: 4700, loss = 0.3220\n",
      "epoch: 4720, loss = 0.3194\n",
      "epoch: 4740, loss = 0.3169\n",
      "epoch: 4760, loss = 0.3145\n",
      "epoch: 4780, loss = 0.3121\n",
      "epoch: 4800, loss = 0.3096\n",
      "epoch: 4820, loss = 0.3073\n",
      "epoch: 4840, loss = 0.3049\n",
      "epoch: 4860, loss = 0.3026\n",
      "epoch: 4880, loss = 0.3003\n",
      "epoch: 4900, loss = 0.2980\n",
      "epoch: 4920, loss = 0.2957\n",
      "epoch: 4940, loss = 0.2935\n",
      "epoch: 4960, loss = 0.2913\n",
      "epoch: 4980, loss = 0.2891\n",
      "epoch: 5000, loss = 0.2869\n",
      "epoch: 5020, loss = 0.2848\n",
      "epoch: 5040, loss = 0.2827\n",
      "epoch: 5060, loss = 0.2806\n",
      "epoch: 5080, loss = 0.2785\n",
      "epoch: 5100, loss = 0.2765\n",
      "epoch: 5120, loss = 0.2744\n",
      "epoch: 5140, loss = 0.2724\n",
      "epoch: 5160, loss = 0.2705\n",
      "epoch: 5180, loss = 0.2685\n",
      "epoch: 5200, loss = 0.2666\n",
      "epoch: 5220, loss = 0.2647\n",
      "epoch: 5240, loss = 0.2628\n",
      "epoch: 5260, loss = 0.2609\n",
      "epoch: 5280, loss = 0.2591\n",
      "epoch: 5300, loss = 0.2572\n",
      "epoch: 5320, loss = 0.2554\n",
      "epoch: 5340, loss = 0.2536\n",
      "epoch: 5360, loss = 0.2519\n",
      "epoch: 5380, loss = 0.2501\n",
      "epoch: 5400, loss = 0.2484\n",
      "epoch: 5420, loss = 0.2467\n",
      "epoch: 5440, loss = 0.2450\n",
      "epoch: 5460, loss = 0.2433\n",
      "epoch: 5480, loss = 0.2417\n",
      "epoch: 5500, loss = 0.2400\n",
      "epoch: 5520, loss = 0.2384\n",
      "epoch: 5540, loss = 0.2368\n",
      "epoch: 5560, loss = 0.2353\n",
      "epoch: 5580, loss = 0.2337\n",
      "epoch: 5600, loss = 0.2322\n",
      "epoch: 5620, loss = 0.2306\n",
      "epoch: 5640, loss = 0.2291\n",
      "epoch: 5660, loss = 0.2276\n",
      "epoch: 5680, loss = 0.2262\n",
      "epoch: 5700, loss = 0.2247\n",
      "epoch: 5720, loss = 0.2233\n",
      "epoch: 5740, loss = 0.2218\n",
      "epoch: 5760, loss = 0.2204\n",
      "epoch: 5780, loss = 0.2190\n",
      "epoch: 5800, loss = 0.2176\n",
      "epoch: 5820, loss = 0.2163\n",
      "epoch: 5840, loss = 0.2149\n",
      "epoch: 5860, loss = 0.2136\n",
      "epoch: 5880, loss = 0.2123\n",
      "epoch: 5900, loss = 0.2110\n",
      "epoch: 5920, loss = 0.2097\n",
      "epoch: 5940, loss = 0.2084\n",
      "epoch: 5960, loss = 0.2072\n",
      "epoch: 5980, loss = 0.2059\n",
      "epoch: 6000, loss = 0.2047\n",
      "epoch: 6020, loss = 0.2035\n",
      "epoch: 6040, loss = 0.2023\n",
      "epoch: 6060, loss = 0.2011\n",
      "epoch: 6080, loss = 0.1999\n",
      "epoch: 6100, loss = 0.1987\n",
      "epoch: 6120, loss = 0.1976\n",
      "epoch: 6140, loss = 0.1964\n",
      "epoch: 6160, loss = 0.1953\n",
      "epoch: 6180, loss = 0.1942\n",
      "epoch: 6200, loss = 0.1931\n",
      "epoch: 6220, loss = 0.1920\n",
      "epoch: 6240, loss = 0.1909\n",
      "epoch: 6260, loss = 0.1898\n",
      "epoch: 6280, loss = 0.1888\n",
      "epoch: 6300, loss = 0.1877\n",
      "epoch: 6320, loss = 0.1867\n",
      "epoch: 6340, loss = 0.1856\n",
      "epoch: 6360, loss = 0.1846\n",
      "epoch: 6380, loss = 0.1836\n",
      "epoch: 6400, loss = 0.1826\n",
      "epoch: 6420, loss = 0.1816\n",
      "epoch: 6440, loss = 0.1807\n",
      "epoch: 6460, loss = 0.1797\n",
      "epoch: 6480, loss = 0.1788\n",
      "epoch: 6500, loss = 0.1778\n",
      "epoch: 6520, loss = 0.1769\n",
      "epoch: 6540, loss = 0.1760\n",
      "epoch: 6560, loss = 0.1750\n",
      "epoch: 6580, loss = 0.1741\n",
      "epoch: 6600, loss = 0.1732\n",
      "epoch: 6620, loss = 0.1724\n",
      "epoch: 6640, loss = 0.1715\n",
      "epoch: 6660, loss = 0.1706\n",
      "epoch: 6680, loss = 0.1698\n",
      "epoch: 6700, loss = 0.1689\n",
      "epoch: 6720, loss = 0.1681\n",
      "epoch: 6740, loss = 0.1672\n",
      "epoch: 6760, loss = 0.1664\n",
      "epoch: 6780, loss = 0.1656\n",
      "epoch: 6800, loss = 0.1648\n",
      "epoch: 6820, loss = 0.1640\n",
      "epoch: 6840, loss = 0.1632\n",
      "epoch: 6860, loss = 0.1624\n",
      "epoch: 6880, loss = 0.1616\n",
      "epoch: 6900, loss = 0.1609\n",
      "epoch: 6920, loss = 0.1601\n",
      "epoch: 6940, loss = 0.1594\n",
      "epoch: 6960, loss = 0.1586\n",
      "epoch: 6980, loss = 0.1579\n",
      "epoch: 7000, loss = 0.1571\n",
      "epoch: 7020, loss = 0.1564\n",
      "epoch: 7040, loss = 0.1557\n",
      "epoch: 7060, loss = 0.1550\n",
      "epoch: 7080, loss = 0.1543\n",
      "epoch: 7100, loss = 0.1536\n",
      "epoch: 7120, loss = 0.1529\n",
      "epoch: 7140, loss = 0.1522\n",
      "epoch: 7160, loss = 0.1515\n",
      "epoch: 7180, loss = 0.1509\n",
      "epoch: 7200, loss = 0.1502\n",
      "epoch: 7220, loss = 0.1495\n",
      "epoch: 7240, loss = 0.1489\n",
      "epoch: 7260, loss = 0.1482\n",
      "epoch: 7280, loss = 0.1476\n",
      "epoch: 7300, loss = 0.1470\n",
      "epoch: 7320, loss = 0.1463\n",
      "epoch: 7340, loss = 0.1457\n",
      "epoch: 7360, loss = 0.1451\n",
      "epoch: 7380, loss = 0.1445\n",
      "epoch: 7400, loss = 0.1439\n",
      "epoch: 7420, loss = 0.1433\n",
      "epoch: 7440, loss = 0.1427\n",
      "epoch: 7460, loss = 0.1421\n",
      "epoch: 7480, loss = 0.1415\n",
      "epoch: 7500, loss = 0.1409\n",
      "epoch: 7520, loss = 0.1403\n",
      "epoch: 7540, loss = 0.1398\n",
      "epoch: 7560, loss = 0.1392\n",
      "epoch: 7580, loss = 0.1386\n",
      "epoch: 7600, loss = 0.1381\n",
      "epoch: 7620, loss = 0.1375\n",
      "epoch: 7640, loss = 0.1370\n",
      "epoch: 7660, loss = 0.1364\n",
      "epoch: 7680, loss = 0.1359\n",
      "epoch: 7700, loss = 0.1354\n",
      "epoch: 7720, loss = 0.1348\n",
      "epoch: 7740, loss = 0.1343\n",
      "epoch: 7760, loss = 0.1338\n",
      "epoch: 7780, loss = 0.1333\n",
      "epoch: 7800, loss = 0.1328\n",
      "epoch: 7820, loss = 0.1323\n",
      "epoch: 7840, loss = 0.1318\n",
      "epoch: 7860, loss = 0.1313\n",
      "epoch: 7880, loss = 0.1308\n",
      "epoch: 7900, loss = 0.1303\n",
      "epoch: 7920, loss = 0.1298\n",
      "epoch: 7940, loss = 0.1293\n",
      "epoch: 7960, loss = 0.1289\n",
      "epoch: 7980, loss = 0.1284\n",
      "epoch: 8000, loss = 0.1279\n",
      "epoch: 8020, loss = 0.1275\n",
      "epoch: 8040, loss = 0.1270\n",
      "epoch: 8060, loss = 0.1265\n",
      "epoch: 8080, loss = 0.1261\n",
      "epoch: 8100, loss = 0.1256\n",
      "epoch: 8120, loss = 0.1252\n",
      "epoch: 8140, loss = 0.1247\n",
      "epoch: 8160, loss = 0.1243\n",
      "epoch: 8180, loss = 0.1239\n",
      "epoch: 8200, loss = 0.1234\n",
      "epoch: 8220, loss = 0.1230\n",
      "epoch: 8240, loss = 0.1226\n",
      "epoch: 8260, loss = 0.1222\n",
      "epoch: 8280, loss = 0.1217\n",
      "epoch: 8300, loss = 0.1213\n",
      "epoch: 8320, loss = 0.1209\n",
      "epoch: 8340, loss = 0.1205\n",
      "epoch: 8360, loss = 0.1201\n",
      "epoch: 8380, loss = 0.1197\n",
      "epoch: 8400, loss = 0.1193\n",
      "epoch: 8420, loss = 0.1189\n",
      "epoch: 8440, loss = 0.1185\n",
      "epoch: 8460, loss = 0.1181\n",
      "epoch: 8480, loss = 0.1177\n",
      "epoch: 8500, loss = 0.1173\n",
      "epoch: 8520, loss = 0.1170\n",
      "epoch: 8540, loss = 0.1166\n",
      "epoch: 8560, loss = 0.1162\n",
      "epoch: 8580, loss = 0.1158\n",
      "epoch: 8600, loss = 0.1155\n",
      "epoch: 8620, loss = 0.1151\n",
      "epoch: 8640, loss = 0.1147\n",
      "epoch: 8660, loss = 0.1144\n",
      "epoch: 8680, loss = 0.1140\n",
      "epoch: 8700, loss = 0.1136\n",
      "epoch: 8720, loss = 0.1133\n",
      "epoch: 8740, loss = 0.1129\n",
      "epoch: 8760, loss = 0.1126\n",
      "epoch: 8780, loss = 0.1122\n",
      "epoch: 8800, loss = 0.1119\n",
      "epoch: 8820, loss = 0.1115\n",
      "epoch: 8840, loss = 0.1112\n",
      "epoch: 8860, loss = 0.1109\n",
      "epoch: 8880, loss = 0.1105\n",
      "epoch: 8900, loss = 0.1102\n",
      "epoch: 8920, loss = 0.1099\n",
      "epoch: 8940, loss = 0.1095\n",
      "epoch: 8960, loss = 0.1092\n",
      "epoch: 8980, loss = 0.1089\n",
      "epoch: 9000, loss = 0.1086\n",
      "epoch: 9020, loss = 0.1082\n",
      "epoch: 9040, loss = 0.1079\n",
      "epoch: 9060, loss = 0.1076\n",
      "epoch: 9080, loss = 0.1073\n",
      "epoch: 9100, loss = 0.1070\n",
      "epoch: 9120, loss = 0.1067\n",
      "epoch: 9140, loss = 0.1064\n",
      "epoch: 9160, loss = 0.1061\n",
      "epoch: 9180, loss = 0.1058\n",
      "epoch: 9200, loss = 0.1055\n",
      "epoch: 9220, loss = 0.1052\n",
      "epoch: 9240, loss = 0.1049\n",
      "epoch: 9260, loss = 0.1046\n",
      "epoch: 9280, loss = 0.1043\n",
      "epoch: 9300, loss = 0.1040\n",
      "epoch: 9320, loss = 0.1037\n",
      "epoch: 9340, loss = 0.1034\n",
      "epoch: 9360, loss = 0.1031\n",
      "epoch: 9380, loss = 0.1028\n",
      "epoch: 9400, loss = 0.1026\n",
      "epoch: 9420, loss = 0.1023\n",
      "epoch: 9440, loss = 0.1020\n",
      "epoch: 9460, loss = 0.1017\n",
      "epoch: 9480, loss = 0.1014\n",
      "epoch: 9500, loss = 0.1012\n",
      "epoch: 9520, loss = 0.1009\n",
      "epoch: 9540, loss = 0.1006\n",
      "epoch: 9560, loss = 0.1004\n",
      "epoch: 9580, loss = 0.1001\n",
      "epoch: 9600, loss = 0.0998\n",
      "epoch: 9620, loss = 0.0996\n",
      "epoch: 9640, loss = 0.0993\n",
      "epoch: 9660, loss = 0.0990\n",
      "epoch: 9680, loss = 0.0988\n",
      "epoch: 9700, loss = 0.0985\n",
      "epoch: 9720, loss = 0.0983\n",
      "epoch: 9740, loss = 0.0980\n",
      "epoch: 9760, loss = 0.0978\n",
      "epoch: 9780, loss = 0.0975\n",
      "epoch: 9800, loss = 0.0973\n",
      "epoch: 9820, loss = 0.0970\n",
      "epoch: 9840, loss = 0.0968\n",
      "epoch: 9860, loss = 0.0965\n",
      "epoch: 9880, loss = 0.0963\n",
      "epoch: 9900, loss = 0.0960\n",
      "epoch: 9920, loss = 0.0958\n",
      "epoch: 9940, loss = 0.0956\n",
      "epoch: 9960, loss = 0.0953\n",
      "epoch: 9980, loss = 0.0951\n",
      "epoch: 10000, loss = 0.0948\n"
     ]
    }
   ],
   "source": [
    "#Train the Model\n",
    "# ============================ step 5/6 training ============================    \n",
    "for epoch in range(num_epochs):\n",
    "    models.train()\n",
    "    optimizers.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred  = models(X_train)\n",
    "    # Compute Loss\n",
    "    loss = criterions(y_pred, y_train)\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizers.step()\n",
    "    if (epoch+1) % 20 == 0:                                         \n",
    "        # printing loss values on every 10 epochs to keep track\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, \n",
    "+ when you call `models(X_train)`, you automatically call `models.forward()` to propagate forward.\n",
    "+  Next, the loss is calculated. When `loss.backward()` is called, it computes the loss gradient with respect to the weights (of the layer). \n",
    "+ The weights are then updated by calling `optimizer.step()`. \n",
    "+ After this, the weights have to be emptied for the next iteration. So the `zero_grad()` method is called.\n",
    "\n",
    "The above code prints the loss at each 20th epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 7 Model Performance\n",
    "  \n",
    "Let us finally see the model accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9500\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = models(X_test)\n",
    "    y_pred = nn.Softmax(dim=1)(logits)\n",
    "    y_predicted_cls = y_pred.argmax(1)\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy: {acc.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94         9\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.96      0.96      0.96        20\n",
      "weighted avg       0.96      0.95      0.95        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_predicted_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB Assignment\n",
    "\n",
    "<font color=red>DDL: week 11.</font>\n",
    "\n",
    "### Exercise 1 logistic regression\n",
    "This exercise uses dataset digit01.csv , which has 13 columns, and the last column is the dependent variable. \n",
    "\n",
    "This part requires you to implement a `logistic regression` using the pytorch framework (defining a logistic regression class that inherits `nn.module`). To test your model, we provide a dataset `digit01.csv` which is in the **datasets folder**. This dataset requires you to divide the training set and the test set by yourself, and it is recommended that 80% of the training set and 20% of the test set be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2  Handwriting recognition with MLP\n",
    "\n",
    "Like last week's lab , your task in this section is also about recognizing handwritten digits, but you are required to use MLP to complete the exercise. It is recommended that you define an MLP class, which is a subclass of `nn.module`.\n",
    "\n",
    "For this exercise we use the `minist` dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "9cf8428aa180ee23632ed7df20f7a595edda7c60e668686876baf89d702ea1cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#LAB3-tutorial-for-Machine-Learning--Naïve-Bayes-Classifier-Algorithm-\" data-toc-modified-id=\"LAB3-tutorial-for-Machine-Learning--Naïve-Bayes-Classifier-Algorithm--1\"><center>LAB3 tutorial for Machine Learning <br> Naïve Bayes Classifier Algorithm </center></a></span><ul class=\"toc-item\"><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-1.1\">Objective</a></span></li><li><span><a href=\"#1-The-Naïve-Bayes-algorithm\" data-toc-modified-id=\"1-The-Naïve-Bayes-algorithm-1.2\">1 The Naïve Bayes algorithm</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-Bayes'-Theorem:\" data-toc-modified-id=\"1.1-Bayes'-Theorem:-1.2.1\">1.1 Bayes' Theorem:</a></span></li><li><span><a href=\"#1.2-Working-of-Naïve-Bayes'-Classifier:\" data-toc-modified-id=\"1.2-Working-of-Naïve-Bayes'-Classifier:-1.2.2\">1.2 Working of Naïve Bayes' Classifier:</a></span></li><li><span><a href=\"#1.3-naive-assumption\" data-toc-modified-id=\"1.3-naive-assumption-1.2.3\">1.3 naive assumption</a></span></li><li><span><a href=\"#1.4-Gaussian-Naive-Bayes-classifier\" data-toc-modified-id=\"1.4-Gaussian-Naive-Bayes-classifier-1.2.4\">1.4 <strong>Gaussian Naive Bayes classifier</strong></a></span></li><li><span><a href=\"#1.5-Zero-Conditional-Probability\" data-toc-modified-id=\"1.5-Zero-Conditional-Probability-1.2.5\">1.5 Zero Conditional Probability</a></span></li></ul></li><li><span><a href=\"#2-LAB-Assignment\" data-toc-modified-id=\"2-LAB-Assignment-1.3\">2 LAB Assignment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise1-Spam-filtering\" data-toc-modified-id=\"Exercise1-Spam-filtering-1.3.1\">Exercise1 Spam filtering</a></span></li><li><span><a href=\"#1)-Preparing-the-text-data\" data-toc-modified-id=\"1)-Preparing-the-text-data-1.3.2\">1) Preparing the text data</a></span></li><li><span><a href=\"#2)-Creating-word-dictionary\" data-toc-modified-id=\"2)-Creating-word-dictionary-1.3.3\">2) Creating word dictionary</a></span></li><li><span><a href=\"#3)-Feature-Extraction-Process\" data-toc-modified-id=\"3)-Feature-Extraction-Process-1.3.4\">3) Feature Extraction Process</a></span></li><li><span><a href=\"#4)-Training-the-Classifiers\" data-toc-modified-id=\"4)-Training-the-Classifiers-1.3.5\">4) Training the Classifiers</a></span></li><li><span><a href=\"#5)-Implementation-of-the-Naive-Bayes-algorithm\" data-toc-modified-id=\"5)-Implementation-of-the-Naive-Bayes-algorithm-1.3.6\">5) Implementation of the Naive Bayes algorithm</a></span></li><li><span><a href=\"#5)-Checking-the-results-on-test-set\" data-toc-modified-id=\"5)-Checking-the-results-on-test-set-1.3.7\">5) Checking the results on test set</a></span></li><li><span><a href=\"#Exercise-2-Compare-your-Naïve-Bayes-algorithm-with-GassianNB-from-Sklearn,-which-one-does-better?-Where-is-the-gap?\" data-toc-modified-id=\"Exercise-2-Compare-your-Naïve-Bayes-algorithm-with-GassianNB-from-Sklearn,-which-one-does-better?-Where-is-the-gap?-1.3.8\">Exercise 2 Compare your Naïve Bayes algorithm with GassianNB from Sklearn, which one does better? Where is the gap?</a></span></li><li><span><a href=\"#Exercise-3-Questions\" data-toc-modified-id=\"Exercise-3-Questions-1.3.9\">Exercise 3 Questions</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center >LAB3 tutorial for Machine Learning <br> Naïve Bayes Classifier Algorithm </center>\n",
    "> The document description are designed by JIa Yanhong in 2022. Sept. 16th\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- Understand one of the most popular and simple machine learning classification algorithms, the Naive Bayes algorithm\n",
    "\n",
    "- Learn how to implement the Naive Bayes Classifier in Python\n",
    "\n",
    "+ Complete the LAB assignment and submit it to BB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 The Naïve Bayes algorithm\n",
    "\n",
    "The Naïve Bayes algorithm is comprised of two words Naïve and Bayes, Which can be described as:\n",
    "\n",
    "- **Naïve**: It is called Naïve because it assumes that the occurrence of a certain feature is independent of the occurrence of other features. Such as if the fruit is identified on the bases of color, shape, and taste, then red, spherical, and sweet fruit is recognized as an apple. Hence each feature individually contributes to identify that it is an apple without depending on each other.\n",
    "- **Bayes**: It is called Bayes because it depends on the principle of [Bayes' Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem).\n",
    "\n",
    "\n",
    "\n",
    "### 1.1 Bayes' Theorem:\n",
    "\n",
    "- Bayes' theorem is also known as **Bayes' Rule** or **Bayes' law**, which is used to determine the probability of a hypothesis with prior knowledge. It depends on the conditional probability.\n",
    "- The formula for Bayes' theorem is given as:\n",
    "  $$ P(A|B)=\\frac{P(B|A)P(A)}{P(B)} $$\n",
    " \n",
    "\n",
    "**Where,**\n",
    "\n",
    "**P(A|B) is Posterior probability**: Probability of hypothesis A on the observed event B.\n",
    "\n",
    "**P(B|A) is Likelihood probability**: Probability of the evidence given that the probability of a hypothesis is true.\n",
    "\n",
    "**P(A) is Prior Probability**: Probability of hypothesis before observing the evidence.\n",
    "\n",
    "**P(B) is Marginal Probability**: Probability of Evidence.\n",
    "\n",
    "$$ posterior=\\frac{likelihood  \\times prior}{evidence} $$\n",
    "\n",
    "In practice, there is interest only in the numerator of that fraction, because the denominator does not depend on C and the values of the features $x_{i}$ are given, so that the denominator is effectively constant. The numerator is equivalent to the joint probability model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Working of Naïve Bayes' Classifier:\n",
    "\n",
    "Working of Naïve Bayes' Classifier can be understood with the help of the below example:\n",
    "\n",
    "Suppose we have a dataset of **weather conditions** and corresponding target variable \"**Play**\". So using this dataset we need to decide that whether we should play or not on a particular day according to the weather conditions. So to solve this problem, we need to follow the below steps:\n",
    "\n",
    "1. Convert the given dataset into frequency tables.\n",
    "\n",
    "2. Generate Likelihood table by finding the probabilities of given features.\n",
    "\n",
    "3. Now, use Bayes theorem to calculate the posterior probability.\n",
    "\n",
    "   **Problem**: If the weather is sunny, then the Player should play or not?\n",
    "\n",
    "   **Solution**: To solve this, first consider the below dataset:\n",
    "\n",
    "   <center>\n",
    "\n",
    "   |   Day     | Outlook  | Play |\n",
    "   | :----- | :------- | :--- |\n",
    "   | **0**  | Rainy    | Yes  |\n",
    "   | **1**  | Sunny    | Yes  |\n",
    "   | **2**  | Overcast | Yes  |\n",
    "   | **3**  | Overcast | Yes  |\n",
    "   | **4**  | Sunny    | No   |\n",
    "   | **5**  | Rainy    | Yes  |\n",
    "   | **6**  | Sunny    | Yes  |\n",
    "   | **7**  | Overcast | Yes  |\n",
    "   | **8**  | Rainy    | No   |\n",
    "   | **9**  | Sunny    | No   |\n",
    "   | **10** | Sunny    | Yes  |\n",
    "   | **11** | Rainy    | No   |\n",
    "   | **12** | Overcast | Yes  |\n",
    "   | **13** | Overcast | Yes  |\n",
    "\n",
    "  </center>\n",
    "  \n",
    "   **Frequency table for the Weather Conditions:**\n",
    "   \n",
    "  <center>\n",
    "\n",
    "   | Weather  | Yes  | No   |\n",
    "   | -------- | ---- | ---- |\n",
    "   | Overcast | 5    | 0    |\n",
    "   | Rainy    | 2    | 2    |\n",
    "   | Sunny    | 3    | 2    |\n",
    "   | Total    | 10   | 5    |\n",
    "   \n",
    "  </center>\n",
    "\n",
    "   **Likelihood table weather condition:**\n",
    "   <center>\n",
    "\n",
    "   | Weather  | No        | Yes        |            |\n",
    "   | -------- | --------- | ---------- | ---------- |\n",
    "   | Overcast | 0/4         | 5/10          | 5/14= 0.35 |\n",
    "   | Rainy    | 2/4         | 2/10          | 4/14=0.29  |\n",
    "   | Sunny    | 2/4         | 3/10          | 5/14=0.35  |\n",
    "   | All      | 4/14=0.29 | 10/14=0.71 |            |\n",
    "   \n",
    "  </center>\n",
    "  \n",
    "   **Applying Bayes'theorem:**\n",
    "\n",
    "   > **P(Yes|Sunny)= P(Sunny|Yes)\\*P(Yes)/P(Sunny)**\n",
    "   > \n",
    "   > P(Sunny|Yes)= 3/10= 0.3\n",
    "   > \n",
    "   > P(Sunny)= 0.35\n",
    "   > \n",
    "   > P(Yes)=0.71\n",
    "   > \n",
    "   So \n",
    "   > P(Yes|Sunny) = 0.3*0.71/0.35= **0.60**\n",
    "\n",
    "   > **P(No|Sunny)= P(Sunny|No)\\*P(No)/P(Sunny)**\n",
    "   > \n",
    "   > P(Sunny|NO)= 2/4=0.5\n",
    "   > \n",
    "   > P(No)= 0.29\n",
    "   > \n",
    "   > P(Sunny)= 0.35\n",
    "\n",
    "   So \n",
    "   > P(No|Sunny)= 0.5*0.29/0.35 = **0.41**\n",
    "\n",
    "   So as we can see from the above calculation that **P(Yes|Sunny)>P(No|Sunny)**\n",
    "\n",
    "   **Hence on a Sunny day, Player can play the game.**\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 naive assumption\n",
    "\n",
    "\n",
    "\n",
    "To start with, let us consider a dataset.\n",
    "\n",
    "Consider a fictional dataset that describes the weather conditions for playing a game of golf. Given the weather conditions, each tuple classifies the conditions as fit(“Yes”) or unfit(“No”) for plaing golf.\n",
    "\n",
    "Here is a tabular representation of our dataset.\n",
    "\n",
    "\n",
    "<center>\n",
    "\n",
    "| Day  | OUTLOOK  | TEMPERATURE | HUMIDITY | WINDY | PLAY GOLF |\n",
    "| :--: | :------: | :---------: | :------: | :---: | :-------: |\n",
    "|  0   |  Rainy   |     Hot     |   High   | False |    No     |\n",
    "|  1   |  Rainy   |     Hot     |   High   | True  |    No     |\n",
    "|  2   | Overcast |     Hot     |   High   | False |    Yes    |\n",
    "|  3   |  Sunny   |    Mild     |   High   | False |    Yes    |\n",
    "|  4   |  Sunny   |    Cool     |  Normal  | False |    Yes    |\n",
    "|  5   |  Sunny   |    Cool     |  Normal  | True  |    No     |\n",
    "|  6   | Overcast |    Cool     |  Normal  | True  |    Yes    |\n",
    "|  7   |  Rainy   |    Mild     |   High   | False |    No     |\n",
    "|  8   |  Rainy   |    Cool     |  Normal  | False |    Yes    |\n",
    "|  9   |  Sunny   |    Mild     |  Normal  | False |    Yes    |\n",
    "|  10  |  Rainy   |    Mild     |  Normal  | True  |    Yes    |\n",
    "|  11  | Overcast |    Mild     |   High   | True  |    Yes    |\n",
    "|  12  | Overcast |     Hot     |  Normal  | False |    Yes    |\n",
    "|  13  |  Sunny   |    Mild     |   High   | True  |    No     |\n",
    "\n",
    "</center>\n",
    "\n",
    "The dataset is divided into two parts, namely, **feature matrix** and the **response vector**.\n",
    "\n",
    "- Feature matrix contains all the vectors(rows) of dataset in which each vector consists of the value of **dependent features**. In above dataset, features are ‘Outlook’, ‘Temperature’, ‘Humidity’ and ‘Windy’.\n",
    "\n",
    "- Response vector contains the value of **class variable**(prediction or output) for each row of feature matrix. In above dataset, the class variable name is ‘Play golf’.\n",
    "\n",
    "  \n",
    "\n",
    "Now, its time to put a naive assumption to the Bayes’ theorem, which is, **independence** among the features. So now, we split **evidence** into the independent parts.\n",
    "\n",
    "Now, if any two events A and B are independent, then,\n",
    "$$ P(A,B)=P(A)P(B)$$\n",
    "\n",
    "\n",
    "\n",
    "Hence, we reach to the result:\n",
    "\n",
    "$$ P(y|x_1,...,x_n) = \\frac{ P(x_1|y)P(x_2|y)...P(x_n|y)P(y)}{P(x_1)P(x_2)...P(x_n)}$$\n",
    "\n",
    "which can be expressed as:\n",
    "\n",
    "$$P(y|x_1,...,x_n) = \\frac{P(y)\\prod_{i=1}^{n}P(x_i|y)}{P(x_1)P(x_2)...P(x_n)}$$\n",
    "\n",
    "Now, as the denominator remains constant for a given input, we can remove that term:\n",
    "$$ P(y|x_1,...,x_n)\\propto P(y)\\prod_{i=1}^{n}P(x_i|y)$$\n",
    "\n",
    "\n",
    "Now, we need to create a classifier model. For this, we find the probability of given set of inputs for all possible values of the class variable *y* and pick up the output with maximum probability. This can be expressed mathematically as:\n",
    "$$ y = argmax_{y} P(y)\\prod_{i=1}^{n}P(x_i|y)$$\n",
    "\n",
    "So, finally, we are left with the task of calculating P(y) and $P(x_{i}|y)$.\n",
    "\n",
    "> Please note that P(y) is also called **class probability** and $P(x_{i}|y)$ is called **conditional probability**.\n",
    "\n",
    "The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $P(x_{i}|y)$.\n",
    "\n",
    "Let us try to apply the above formula manually on our weather dataset. For this, we need to do some precomputations on our dataset.\n",
    "\n",
    "We need to find $P(x_{i}|y_{i})$ for each $x_{i}$ in X and $y_{i}$ in y. All these calculations have been demonstrated in the tables below:\n",
    "<center ><img src=\"images/naive-bayes-classification.png\" alt=\"table23 \" style=\"zoom:100%;\" /></center>\n",
    "\n",
    "\n",
    "So, in the figure above, we have calculated $P(x_{i}|y_{i})$ for each xi in X and $y_{i}$ in y manually in the tables 1-4. For example, probability of playing golf given that the temperature is cool, i.e  `P(temp. = cool | play golf = Yes) = 3/9`.\n",
    "\n",
    "Also, we need to find class probabilities (P(y)) which has been calculated in the table 5. For example, P(play golf = Yes) = 9/14.\n",
    "\n",
    "So now, we are done with our pre-computations and the classifier is ready!\n",
    "\n",
    "Let us test it on a new set of features (let us call it today):\n",
    "$$ today = (Sunny, Hot, Normal, False)$$\n",
    "\n",
    "So, probability of playing golf is given by:\n",
    "$$P(Yes | today) = \\frac{P(Sunny\\ Outlook|Yes)P(Hot\\ Temperature|Yes)P(Normal\\ Humidity|Yes)P(No\\ Wind|Yes)P(Yes)}{P(today)})$$\n",
    "\n",
    "\n",
    "\n",
    "and probability to not play golf is given by:\n",
    "$$P(No | today) = \\frac{P(Sunny\\ Outlook|No)P(Hot\\ Temperature|No)P(Normal\\ Humidity|No)P(No\\ Wind|No)P(No)}{P(today)}$$\n",
    "\n",
    "\n",
    "Since, P(today) is common in both probabilities, we can ignore P(today) and find proportional probabilities as:\n",
    "\n",
    "$$ P(Yes | today) \\propto \\frac{2}{9}.\\frac{2}{9}.\\frac{6}{9}.\\frac{6}{9}.\\frac{9}{14} \\approx 0.0141 $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ P(No | today) \\propto \\frac{3}{5}.\\frac{2}{5}.\\frac{1}{5}.\\frac{2}{5}.\\frac{5}{14} \\approx 0.0068 $$\n",
    "\n",
    "Now, since\n",
    "$$P(Yes | today) + P(No | today) = 1 $$\n",
    "\n",
    "\n",
    "These numbers can be converted into a probability by making the sum equal to 1 (normalization):\n",
    "\n",
    "$$ P(Yes | today) = \\frac{0.0141}{0.0141 + 0.0068} = 0.67 $$\n",
    "and\n",
    "\n",
    "$$ P(No | today) = \\frac{0.0068}{0.0141 + 0.0068} = 0.33 $$\n",
    "\n",
    "Since\n",
    "\n",
    "$$ P(Yes | today) > P(No | today) $$\n",
    "\n",
    "So, prediction that golf would be played is ‘Yes’.\n",
    "\n",
    "The method that we discussed above is applicable for discrete data. In case of continuous data, we need to make some assumptions regarding the distribution of values of each feature. The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of P(xi | y).\n",
    "\n",
    "Now, we discuss one of such classifiers here.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 **Gaussian Naive Bayes classifier**\n",
    "\n",
    "In Gaussian Naive Bayes, continuous values associated with each feature are assumed to be distributed according to a **Gaussian distribution**. A Gaussian distribution is also called [Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution). When plotted, it gives a bell shaped curve which is symmetric about the mean of the feature values as shown below:\n",
    "\n",
    "<center><img src=\"images/naive-bayes-classification-1.png\" alt=\"normal \" style=\"zoom:80%;\" /></center>\n",
    "\n",
    "The likelihood of the features is assumed to be Gaussian, hence, conditional probability is given by:\n",
    "\n",
    "$$ P(x_i | y) = \\frac{1}{\\sqrt{2\\pi\\sigma _{y}^{2} }} exp \\left (-\\frac{(x_i-\\mu _{y})^2}{2\\sigma _{y}^{2}}  \\right )$$\n",
    "\n",
    "<font size=5>Example: Continuous-valued Features</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "+ <font size=5>Temperature is naturally of continuous value</font>\n",
    "> Yes: 25.2, 19.3, 18.5, 21.7, 20.1, 24.3, 22.8, 23.1, 19.8\n",
    "> \n",
    "> No: 27.3, 30.1, 17.4, 29.5, 15.1\n",
    "\n",
    "+ <font size=5>Estimate mean and variance for each class</font>\n",
    "\n",
    "$$ \\mu =\\frac{1}{N}\\sum_{1}^{N}x_{n}\\ ,\\  \\sigma ^{2} =\\frac{1}{N}\\sum_{1}^{N}(x_{n}-\\mu )^{2}$$\n",
    "\n",
    "\n",
    "> $ \\mu _{Yes}=21.64\\ , \\ \\sigma _{Yes}=2.35$\n",
    "> \n",
    "> $\\mu _{No}=23.88\\ , \\ \\sigma _{No}=7.09 $\n",
    "\n",
    "\n",
    "+ <font size=5>Learning Phase: output two Gaussian models </font>\n",
    "> $\\hat{P}(x | Yes) = \\frac{1}{2.35\\sqrt{2\\pi }} exp \\left (-\\frac{(x-21.64)^2}{2\\times 2.35^{2}}  \\right )$\n",
    "> \n",
    ">$\\hat{P}(x | No) = \\frac{1}{7.09\\sqrt{2\\pi }} exp \\left (-\\frac{(x-23.88)^2}{2\\times 7.09^{2}}  \\right )$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Zero Conditional Probability\n",
    "Sometimes, we face a zero conditional probability problem during test.\n",
    "\n",
    "$$\\hat{P}(x_{1} |c_{i})\\cdots \\hat{P}(a_{jk} |c_{i})\\cdots \\hat{P}(x_{n} |c_{i})=0\\ for\\ x_{i}=a_{jk},\\ \\hat{P}(a_{jk} |c_{i})=0$$\n",
    "\n",
    "Let's test another example:\n",
    "$$ today = (Overcast, Hot, Normal, False)$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "and probability to not play golf is given by:\n",
    "$$\\begin{aligned} \n",
    "P(No | today) &\\propto P(Overcast\\ Outlook|No)P(Hot\\ Temperature|No)P(Normal\\ Humidity|No)P(No\\ Wind|No)P(No)\\\\\n",
    "&\\propto\\frac{0}{5}.\\frac{2}{5}.\\frac{1}{5}.\\frac{2}{5}.\\frac{5}{14} = 0\\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "In this case, the model becomes ineffective!!!\n",
    "\n",
    "For a remedy, class conditional probabilities reestimated with:\n",
    "$$\\hat{P}(a_{jk} |c_{i})=\\frac{n_{c}+mp}{n+m}$$\n",
    "\n",
    "$n_{c}$: number of\t\ttraining examples for which $x_{j}=a_{jk}$ and $c=c_{i}$ \n",
    " \n",
    "n : number of\ttraining examples for which $c=c_{i}$\n",
    "\n",
    "p : prior estimate (usually,$p=\\frac{1}{t}$\tfor t possible\tvalues of\t$x_{j} $)\n",
    "\n",
    "m : weight to prior (number of \" virtual\" examples,\t$m\\geqslant 1$)+ <font size=5>Temperature is naturally of continuous value</font>\n",
    "\n",
    "+ <font size=5>Example: P(outlook=overcast|no)=0 in the play-golf dataset</font>\n",
    "\n",
    "  + Adding m `virtual` examples\n",
    "    +   In this dataset,  training examples for the `no` class is 5.\n",
    "    + We can only add m=1 `virtual` example in our m-esitmate remedy.\n",
    "  + The `outlook` feature can takes only 3 values. So p=1/3.\n",
    "  + Re-estimate P(outlook|no) with the m-estimate\n",
    "\n",
    "\n",
    "> $p(overcast|no)=\\frac{0+1*\\frac{1}{3}}{5+1}=\\frac{1}{18}$\n",
    "> \n",
    ">$p(sunny|no)=\\frac{3+1*\\frac{1}{3}}{5+1}=\\frac{5}{9}$\n",
    "\n",
    ">$p(rain|no)=\\frac{2+1*\\frac{1}{3}}{5+1}=\\frac{7}{18}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 LAB Assignment\n",
    "\n",
    "\n",
    "Text mining (deriving information from text) is a wide field which has gained popularity with the huge text data being generated. Automation of a number of applications like sentiment analysis, document classification,\n",
    "topic classification, text summarization, machine translation, etc., has been\n",
    "done using machine learning models.  \n",
    "\n",
    "### Exercise1 Spam filtering\n",
    "\n",
    "<font color=red>In this Exercise, you are required to write your spam filter by using naïve Bayes method. This time you should not use 3rd party libraries including scikit-learn.</font>\n",
    "\n",
    "Spam filtering is a beginner’s example of the document classification task which involves classifying an email as spam or non-spam (a.k.a. ham) mail. An email dataset will be provided. We will use the following steps to build this application:\n",
    "1) Preparing the text data\n",
    "2) Creating a word dictionary\n",
    "3) Feature extraction\n",
    "4) Implementation of the Naive Bayes algorithm\n",
    "5) Training the classifier\n",
    "6) Checking the results on the test set\n",
    "### 1) Preparing the text data\n",
    "The data-set used here, is split into a training set and a test set containing 702 mails and 260 mails respectively, divided equally between spam and ham mails. You will easily recognize spam mails as it contains `spmsg` in its filename.\n",
    "\n",
    "In any text mining problem, text cleaning is the first step where we remove those words from the document which may not contribute to the information we want to extract.  Emails may contain a lot of undesirable characters like punctuation marks, stop words, digits, etc which may not be helpful in detecting the spam email. The emails in Ling-spam corpus have been already preprocessed in the following ways:\n",
    "\n",
    "1. **Removal of stop words** – Stop words like “and”, “the”, “of”, etc are very common in all English sentences and are not very meaningful in deciding spam or legitimate status, so these words have been removed from the emails.\n",
    "\n",
    "2. **Lemmatization** – It is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. For example, “include”, “includes,” and “included” would all be represented as “include”. The context of the sentence is also preserved in lemmatization as opposed to stemming (another buzz word in text mining which does not consider meaning of the sentence)\n",
    "\n",
    "We still need to remove the non-words like punctuation `marks` or `special characters` from the mail documents. There are several ways to do it. Here, we will remove such words after creating a dictionary, which is a very convenient method to do so since when you have a dictionary; you need to remove every such word only once.\n",
    "### 2) Creating word dictionary\n",
    "We will only perform text analytics on the content to detect the spam mails. As the first step, we need to create a dictionary of words and their frequency. For this task, a training set of 700 mails is utilized. This python function will create the dictionary for you.\n",
    "```Python\n",
    "from collections import Counter\n",
    "def make_Dictionary(train_dir):\n",
    "    emails = [os.path.join(train_dir,f) for f in os.listdir (train_dir)]\n",
    "    all_words = []\n",
    "    for mail in emails:\n",
    "        with open (mail) as m:\n",
    "            for i,line in enumerate (m) :\n",
    "                if i == 2:\n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    "    dictionary = Counter(all_words)\n",
    "    # Write code for non-word removal here\n",
    "    \n",
    "    return dictionary\n",
    "```\n",
    "\n",
    "Once the dictionary is created we can add just a few lines of code written below to the above function to remove non-words. Absurd single characters in the dictionary which are irrelevant here are also removed. Do not forget to insert the below code in the function of make_Dictionary:\n",
    "```python\n",
    "list_to_remove = list(dictionary.keys())\n",
    "for item in list_to_remove:\n",
    "    if item.isalpha() == False:\n",
    "        del dictionary[item]\n",
    "    elif len(item) == 1:\n",
    "        del dictionary[item]\n",
    "dictionary = dictionary.most_common(3000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary can be seen by the command “print dictionary”. You may find some absurd word counts to be high but don’t worry, it’s just a dictionary and you always have a chance to improve it later. If you use the provided dataset, make sure your dictionary has some of the entries given below as most frequent words. Here 3000 most frequently used words are chosen in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show the most frequent words in train-mails\n",
    "dictionary = make_Dictionary('ling-spam/train-mails')\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/dictionary.png\" alt=\"normal \" style=\"zoom:80%;\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Feature Extraction Process\n",
    "Once the dictionary is ready, we can extract word count vector (our feature here) of 3000 dimensions for each email of the training set. Each **word count vector** contains the frequency of 3000 words in the training file. Of course you might have guessed by now that most of them will be zero. Let us take an example. Suppose we have 500 words in our dictionary. Each word count vector contains the frequency of 500 dictionary words in the training file. Suppose the text in the training file is “Get the work done, work done”, then it will be encoded as $$[0,0,0,0,0,…….0,0,2,0,0,0,……,0,0,1,0,0,…0,0,1,0,0,……2,0,0,0,0,0]$$ Here, all the word counts are placed at the 296th, 359th, 415th, 495th elements of the word count vector in the length of 500 and the rest are zero.\n",
    "\n",
    "The below python code will generate a feature vector matrix whose rows denote 700 files of the training set and columns denote 3000 words of the dictionary. The value at index ${ij}$ will be the number of occurrences of the $j^{th}$ word of the dictionary in the $i^{th}$ file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "def extract_features(mail_dir):\n",
    "    files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "    features_matrix = np.zeros((len(files),3000))\n",
    "    docID = 0\n",
    "    _i = 0\n",
    "    print(len(files))\n",
    "    for fil in files:\n",
    "        _i+=1\n",
    "        with open(fil) as fi:\n",
    "            for i,line in enumerate(fi):\n",
    "                if i == 2:\n",
    "                    words = line.split()\n",
    "                    for word in words:\n",
    "                        wordID = 0\n",
    "                        for i,d in enumerate(dictionary):\n",
    "                            if d[0] == word:\n",
    "                                wordID = i\n",
    "                                features_matrix[docID,wordID]+=1\n",
    "            docID = docID + 1\n",
    "        print('\\r','done {} files'.format(_i),flush=True,end='')\n",
    "    return features_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of words with its frequency\n",
    "features_matrix = extract_features(\"ling-spam/train-mails\")\n",
    "features_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Training the Classifiers\n",
    "Here you should write your Naïve Bayes classifiers after fully understanding its principle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "# Prepare feature vectors per training mail and its labels\n",
    "\n",
    "X_train = [[]]\n",
    "\n",
    "y_train = []\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "# Prepare feature vectors per testing mail and its labels\n",
    "X_test = [[]]\n",
    "y_test = []\n",
    "\n",
    "\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Implementation of the Naive Bayes algorithm\n",
    "\n",
    "Complete the code for naive Bayes algorithm in `predict` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########### Write Your Code Here ###########\n",
    "class NaiveBayes():\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = y.astype(int)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.classes = np.unique(y)\n",
    "        self.parameters = {}\n",
    "        for i, c in enumerate(self.classes):\n",
    "            # Calculate the mean, variance, prior probability of each class\n",
    "            X_Index_c = X[np.where(y == c)]\n",
    "            X_index_c_mean = np.mean(X_Index_c, axis=0, keepdims=True)\n",
    "            X_index_c_var = np.var(X_Index_c, axis=0, keepdims=True)\n",
    "            parameters = {\"mean\": X_index_c_mean, \"var\": X_index_c_var, \"prior\": X_Index_c.shape[0] / X.shape[0]}\n",
    "            self.parameters[\"class\" + str(c)] = parameters\n",
    "\n",
    " \n",
    "\n",
    "    def predict(self, X):\n",
    "        # return class with highest probability\n",
    "        prediction = 0\n",
    "        \n",
    "        # Complete code for naive Bayes algorithm\n",
    "       \n",
    "\n",
    "\n",
    "       \n",
    "        return prediction\n",
    "\n",
    "\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Call the Naive Bayes algorithm, which we wrote ourselves\n",
    "model = NaiveBayes()\n",
    "model.fit(X_train,y_train) \n",
    "result = model.predict(X_test)\n",
    "print (confusion_matrix(y_test, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Checking the results on test set\n",
    "The test set contains 130 spam emails and 130 non-spam emails. Please compute accuracy, recall, F-1 score to evaluate the performance of your spam filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 Compare your Naïve Bayes algorithm with GassianNB from Sklearn, which one does better? Where is the gap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 Questions\n",
    "1. Describe another real-world application where the naïve Bayes method can be applied\n",
    "2. What are the strengths of the naïve Bayes method; when does it perform well?\n",
    "3. What are the weaknesses of the naïve Bayes method; when does it perform poorly?\n",
    "4. What makes the naïve Bayes method a good candidate for the classification problem, if you have enough knowledge about the data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.837px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "773bfaa0e82744962f3138a2d7b2f007250d49f330da41a556809ccbcf17bfbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
